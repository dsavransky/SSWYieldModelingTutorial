{"cells":[{"cell_type":"markdown","id":"bEL-WvZ_ayO1","metadata":{"id":"bEL-WvZ_ayO1"},"source":["# Yield Modeling Tutorial\n","Dmitry Savransky (Cornell) and Rhonda Morgan (JPL)\n","\n","Run the [SSW2024_YieldModelingTutorial1_Setup](https://colab.research.google.com/drive/1ubHBqz6Gcayzri9oMaVCbLvJJx_Nwk_B?usp=sharing) notebook to download the data. The setup notebook needs to just be run **once** once for Hands-on Session IV."]},{"cell_type":"markdown","id":"ZcSPl-Cka3Wa","metadata":{"id":"ZcSPl-Cka3Wa"},"source":["## Google Colab Usage\n","*Please read (don't just hit run) the information given above each code cell as there are separate install cells for Colab*\n","&#128992;\n","*and running Python on your computer*\n","&#128309;.\n","\n","**Confirm login account**\n","* Please make sure to be logged in with the Google account you want to use for the exercises before running the code cells below. You can check by clicking the circular account icon in the top right corner of the colab notebook.\n","\n","**Working directory**\n","* Note: The software and data will be installed in a directory called \"SSW2024/SSWYieldModelingTutorial\" in your Google drive. This directory will be created if it does not exist.\n","\n","**Running cells**\n","* Run cells individually by clicking on the triangle on each cell\n","\n","**To Restart runtime**\n","*   Click on Runtime menu item\n","*   Select Restart session\n","*   Select Run code cells individually from the top\n","\n","**To Recreate runtime**\n","*   Click on Runtime menu item\n","*   Select Disconnect and Delete runtime\n","*   Select Run code cells individually from the top\n","\n","**To Exit:**\n","*   Close the browser window"]},{"cell_type":"markdown","id":"add1c8e0-e4d9-41bf-80a8-ae1884ae397e","metadata":{"id":"add1c8e0-e4d9-41bf-80a8-ae1884ae397e"},"source":["# How to Use This Tutorial\n","\n","This notebook includes markdown cells (such as this one), which provide useful information and narration of what is happening, and code cells (such as the next one), which actually do things. To execute a code cell, click in the cell and then hit Shift+Enter. Alternatively, click within a cell and hit the Run button (play symbol). Depending on how you are running the notebook (via Google Colab or offline, and further depending on your installed version of Jupyter and/or Jupyter-lab), you may see slightly different options. The play symbol may appear in each cell, or above the cells in the toolbar.\n","\n","When a cell completes executing, a number will appear (or update) on the left-hand side of the cell. You can also execute some or all of the cells via options in the 'Runtime' menu, above.  Again, depending on how you are running this notebook you may have either a 'Run' or 'Runtime' menu, but both will have similar options for executing chunks of the code.\n","\n","**Important**: Note that some of the code blocks include exercises, with incomplete code, so you can't just run the whole notebook. You are strongly encouraged to go through the blocks one by one, in the order given.  Exercises do not need to be fully completed before proceeding to the next block."]},{"cell_type":"markdown","id":"kOGZXwQ3bBt-","metadata":{"id":"kOGZXwQ3bBt-"},"source":["## &#128992; Setup Google Drive directory"]},{"cell_type":"markdown","id":"lKTP9aUCbCJE","metadata":{"id":"lKTP9aUCbCJE"},"source":["#### &#128992; **Run the next cell to mount the Google Drive**"]},{"cell_type":"code","execution_count":null,"id":"_J6JDFZUbI-k","metadata":{"id":"_J6JDFZUbI-k"},"outputs":[],"source":["# You will be prompted to Permit this notebook to access your Google Drive files - Click on \"Connect to Google Drive\"\n","# You will then be prompted to Choose an account - click on your preferred Google account\n","# You will then confirm that Google Drive for desktop wants to access your Google Account - scroll to click \"Allow\"\n","# You may need to allow all permissions for this to work (click 'Allow All')\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","id":"cEpRmRIfbMis","metadata":{"id":"cEpRmRIfbMis"},"source":["#### &#128992; **Run the next cell to define the Yield_Modeling directory that was created in the Yield Modeling Setup Noteobook**"]},{"cell_type":"code","execution_count":null,"id":"sFwVl1Z0bP3u","metadata":{"id":"sFwVl1Z0bP3u"},"outputs":[],"source":["# This cell should *only* be executed if running the notebook in Google Colab\n","# SSW Yield Modeling Tutorial\n","ym_dir = 'SSW2024/Yield_Modeling' #@param {type:\"string\"}"]},{"cell_type":"markdown","id":"VguVKE-obVLy","metadata":{"id":"VguVKE-obVLy"},"source":["#### &#128992; **Run the next cell to change to the YieldModelingTutorial directory and install the tutorial and then change directory back to Yield_Modeling**"]},{"cell_type":"code","execution_count":null,"id":"DBntQLYvbaoO","metadata":{"id":"DBntQLYvbaoO"},"outputs":[],"source":["# This cell should *only* be executed if running the notebook in Google Colab\n","# Create the YMT_dir directory in drive\n","import os\n","\n","# Google top level drive dir\n","drive_dir = \"/content/drive/MyDrive/\"\n","\n","# YM_dir directory path\n","ym_path = os.path.join(drive_dir, ym_dir)\n","\n","# YMT_dir\n","YMT_dir = \"SSWYieldModelingTutorial\"\n","\n","# YMT_path\n","YMT_path = os.path.join(ym_path, YMT_dir)\n","\n","# Change to the YMT_path\n","os.chdir(YMT_path)\n","\n","# Install the tutorial backend and requirements - this can also take a little while\n","!pip install -e .\n","\n","# Add SSWYieldModelingtutorial to your system path\n","import sys\n","sys.path.insert(1, YMT_path)\n","\n","# Change directory back to ym_dir\n","os.chdir(ym_path)"]},{"cell_type":"markdown","id":"r8On4Kufbj4d","metadata":{"id":"r8On4Kufbj4d"},"source":["## &#128992; Import jupyter widget for Colab\n","\n","**Run the next cell**"]},{"cell_type":"code","execution_count":null,"id":"Iu85wz45brWA","metadata":{"id":"Iu85wz45brWA"},"outputs":[],"source":["# need to import third party jupyter widget\n","from google.colab import output\n","output.enable_custom_widget_manager()"]},{"cell_type":"markdown","id":"rF-4bV4Zbv6Q","metadata":{"id":"rF-4bV4Zbv6Q"},"source":["## &#128309; Change to the working directory for Python\n","\n","**Run the next cell**"]},{"cell_type":"code","execution_count":null,"id":"BaGfvc7Ub0uC","metadata":{"id":"BaGfvc7Ub0uC"},"outputs":[],"source":["# Change to you the Yield_Modeling directory\n","# For example:  '/Users/jsmith/SSW2024/Yield_Modeling'\n","import os\n","absolute_path_to_the_data_folder_on_your_machine = '' # Please complete!\n","os.chdir(absolute_path_to_the_data_folder_on_your_machine)\n","\n","# SSWYieldModelingTutorial direcctory (from the git clone in Setup notebook)\n","# For example: '/Users/jsmith/SSW2024/Yield_Modeling/SSWYieldModelingTutorial'\n","YMT_path = '' # Please complete!\n","import sys\n","sys.path.append(YMT_path)"]},{"cell_type":"markdown","id":"ROyRKwIdb7td","metadata":{"id":"ROyRKwIdb7td"},"source":["#  &#128992;  &#128309;  The rest of the notebook is for Colab or Python"]},{"cell_type":"markdown","id":"29aff47f-b7db-4cbd-921d-d0cdbccea78c","metadata":{"id":"29aff47f-b7db-4cbd-921d-d0cdbccea78c"},"source":["# Tutorial Setup"]},{"cell_type":"code","execution_count":null,"id":"57cf2f58-23a9-4477-8644-b83234b62816","metadata":{"id":"57cf2f58-23a9-4477-8644-b83234b62816"},"outputs":[],"source":["# This is the start of the tutorial.  All users should execute all cells starting with this one\n","# import all required modules\n","import numpy as np\n","import astropy.units as u\n","from astropy.coordinates import SkyCoord\n","from astropy.time import Time\n","from sympy import symbols, cos, sin, Matrix, simplify, sqrt, solve\n","import matplotlib.pyplot as plt\n","import ipywidgets as widgets\n","import warnings\n","from matplotlib import ticker\n","import matplotlib.colors\n","import scipy\n","from SSWYieldModelingTutorial import SSWYieldModelingTutorial"]},{"cell_type":"code","execution_count":null,"id":"c9bc93ca-0b34-41e5-828d-6b40f7eb8f55","metadata":{"id":"c9bc93ca-0b34-41e5-828d-6b40f7eb8f55"},"outputs":[],"source":["# set up plotting\n","%matplotlib widget\n","plt.rcParams.update({'figure.max_open_warning': 0})"]},{"cell_type":"markdown","id":"bdb00d90-d0b4-4b4c-96d7-d719b31d2267","metadata":{"id":"bdb00d90-d0b4-4b4c-96d7-d719b31d2267"},"source":["# Learning Outcomes\n","\n","This tutorial is intended to introduce the key concepts in science yield modeling for exoplanet direct imaging missions. Upon completion of the tutorial you will be able to:\n","\n","1. Compute the quantities measured by exoplanet imaging instrument\n","2. Estimate the probability of exoplanet detection by an imaging instrument\n","3. Evaluate the impacts of instrument and astrophysical noise sources on exoplanet imaging observations\n","4. Construct basic imaging campaign schedules\n","5. Evaluate the impacts of target availability and other constraints on exoplanet mission yield"]},{"cell_type":"markdown","id":"ba5d2896-2d2d-4caf-bc64-836dc35d8f6b","metadata":{"id":"ba5d2896-2d2d-4caf-bc64-836dc35d8f6b"},"source":["# The Direct Imaging Observables\n","\n","## Derivation of the Governing Equations\n","We wish to compute the direct imaging observable parameters over the course of a planet's orbit.  Typically, we parametrize these observables as the projected separation between star and planet ($s$) and the difference in brightness between planet and star (their flux ratio on a magnitude scale, which we call $\\Delta\\mathrm{mag}$.\n","\n","A two-body (Keplerian) orbit is typically encoded via 6 variables: The semi-major axis ($a$) and eccentricity ($e$) which define the shape of the conic section of the orbit (in our case, all orbits will be elliptical or circular). The orbit's orientation in 3-dimensional space is encoded via three Euler angles: the argument of periapsis ($\\omega$), the inclination ($I$), and the longitude of the ascending node ($\\Omega$). Finally, in order to map time on the orbit to absolute time, we need to define the time of periapsis passage (closest approach between the planet and its host star: $t_p$). The planet's progress along its orbit can then be tracked via time, or any one of several anomaly angles, such as the true anomaly ($\\nu$). From these parameters, we can define the planet's orbital radius magnitude as:\n","$$r = \\frac{a(1 - e^2)}{1 + e\\cos(\\nu)}$$\n","\n","![exoplanet orbit diagram with annotations](../img/exoplanet_orbit_diagram_annotated.png)\n","\n","Let's get Python to produce equations for these starting from first principles."]},{"cell_type":"code","execution_count":null,"id":"5f17a72b-5d4b-4464-ab86-f623f626bb0e","metadata":{"id":"5f17a72b-5d4b-4464-ab86-f623f626bb0e"},"outputs":[],"source":["# define required symbols and the rotation matrices for the orbit's rotation\n","r,I,O,w,nu,th = symbols(\"r,I,Omega,omega,nu,theta\",real=True, positive=True)\n","rot1 = Matrix(([cos(O), sin(O), 0], [-sin(O), cos(O), 0], [0, 0, 1]))\n","rot2 = Matrix(([1, 0, 0], [0, cos(I), sin(I)], [0, -sin(I), cos(I)]))\n","rot3 = Matrix(([cos(w), sin(w), 0], [-sin(w), cos(w), 0], [0, 0, 1]))\n","pCi = rot3*rot2*rot1 # this is the rotation of the perifocal frame in the inertial frame\n","iCp = pCi.transpose() # this is the rotation of the inertial frame in the perifocal frame\n","\n","# we can now define the orbital radius vector in the frame where the observer\n","# is located along the negative third axis\n","r_p = Matrix([r*cos(nu),r*sin(nu),0])\n","r_i = simplify(iCp*r_p)\n","r_i"]},{"cell_type":"markdown","id":"1154cc51-7b6e-4695-be22-7db085f2d2da","metadata":{"id":"1154cc51-7b6e-4695-be22-7db085f2d2da"},"source":["We can simplify things a bit by defining the argument of latitude $\\theta \\triangleq \\nu+\\omega$:"]},{"cell_type":"code","execution_count":null,"id":"a64ec4f4-84a3-4574-825c-ab9acf7a7a02","metadata":{"id":"a64ec4f4-84a3-4574-825c-ab9acf7a7a02"},"outputs":[],"source":["r_i = r_i.subs(nu+w, th)\n","r_i"]},{"cell_type":"markdown","id":"0980fa42-6f30-428a-88fc-a8e9df8b9cb9","metadata":{"id":"0980fa42-6f30-428a-88fc-a8e9df8b9cb9"},"source":["We can now find an expression for the projected separation from the first two components of our orbital radius vector:"]},{"cell_type":"code","execution_count":null,"id":"422e255f-d6fb-4be3-8265-8c6d4add0bb6","metadata":{"id":"422e255f-d6fb-4be3-8265-8c6d4add0bb6"},"outputs":[],"source":["s = simplify(sqrt(r_i[0]**2 + r_i[1]**2))\n","s"]},{"cell_type":"markdown","id":"5ea1d74d-97b8-4c8c-8e97-cec940a4662d","metadata":{"id":"5ea1d74d-97b8-4c8c-8e97-cec940a4662d"},"source":["The $\\Delta\\mathrm{mag}$ is a bit more complicated:\n","$$ \\Delta{\\textrm{mag}} \\triangleq -2.5\\log_{10}\\left(\\frac{F_P}{F_S}\\right) =  -2.5\\log_{10}\\left(p\\Phi(\\beta) \\left(\\frac{R_P}{r}\\right)^2 \\right)$$\n","where $p$ is the geometric albedo of the planet (the fraction of light reflected/scattered by the planet), $R_P$ is the planet's radius, and $Phi(\\beta)$ is the planet's phase function (how the fraction of reflected light changes with the planet's phase angle $\\beta$).  The phase angle is the star-planet-observer angle, and can be approximated as:\n","$$ \\cos\\beta \\approx \\sin I \\sin\\theta$$\n","\n","Note that we can calculate both $s$ and $\\Delta\\mathrm{mag}$ with only a subset of our orbital elements and planetary parameters.  All that we need is $a, e, \\nu, \\omega, I, R_P$.\n","\n","Real planets can have very complex phase functions, based on many parameters whose distributions are not yet well known.  Instead of modeling all of this complexity, we frequently just use the Lambert phase function, which describes an isotropic scatterer (one that reflects light equally in all directions):\n","\n","$$\\pi\\Phi_L(\\beta) = \\sin\\beta + (\\pi - \\beta)\\cos\\beta$$\n","\n","Let's now simulate the observable values over the course of one orbit. We'll define some parameters that are vaguely Earth-like:"]},{"cell_type":"code","execution_count":null,"id":"f0dcc3b2-314b-467c-8dc2-ed8a3412551f","metadata":{"id":"f0dcc3b2-314b-467c-8dc2-ed8a3412551f"},"outputs":[],"source":["# define the orbital elements. Note the use of astropy units to keep things consistent\n","a = 1*u.AU #semi-major axis = 1 AU\n","e = 0.2 # this is actually a pretty high eccentricity\n","I = 90*u.deg # 90 degree inclination - edge-on orbit\n","w = 15*u.deg # 15 degree argument of periapsis\n","R_P = 1*u.earthRad # Earth radius-equivalent planet\n","p = 0.367 # geometric albedo\n","nu = np.linspace(0,2*np.pi,100)*u.rad # The true anomaly varies between 0 and 2\\pi over the course of the orbit"]},{"cell_type":"markdown","id":"249b029f-9341-4f6e-8ea6-7de0afc324dc","metadata":{"id":"249b029f-9341-4f6e-8ea6-7de0afc324dc"},"source":["## Exercise 1\n","Now it's your turn.  Write code to use the variables defined in the previous block to compute the projected separation and $\\Delta\\mathrm{mag}$ over the course of the orbit."]},{"cell_type":"code","execution_count":null,"id":"77988ad0-66a9-440a-b4e0-9de70e312bcc","metadata":{"id":"77988ad0-66a9-440a-b4e0-9de70e312bcc"},"outputs":[],"source":["# compute the observables\n","# Note that these are numerical computations on arrays, so you should be using numpy\n","# methods, such as np.cos, np.sin, np.sqrt, etc.\n","r =       # compute the oribtal radius\n","theta =   # compute the argument of latitude\n","s =       # compute the projected separation\n","beta =    # compute the phase angle\n","Phi =     # compute the value of the Lambert phase function\n","dMag =    # compute the Delta mag\n","\n","# if you get stuck, a reference solution is provided by:\n","# s, dMag = SSWYieldModelingTutorial.calc_s_dMag(a, e, I, w, R_P, p, nu)"]},{"cell_type":"code","source":["# DELETEME\n","s, dMag = SSWYieldModelingTutorial.calc_s_dMag(a, e, I, w, R_P, p, nu)"],"metadata":{"id":"oN2p9wIiSQsi"},"id":"oN2p9wIiSQsi","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"91043ec6-66b5-484e-89ed-b98729870ad0","metadata":{"id":"91043ec6-66b5-484e-89ed-b98729870ad0"},"source":["Now that we have some values, let's plot them:"]},{"cell_type":"code","execution_count":null,"id":"30b7643a-c919-404a-be79-e944876815c6","metadata":{"id":"30b7643a-c919-404a-be79-e944876815c6"},"outputs":[],"source":["# let's plot some values\n","fig1, ax1a = plt.subplots()\n","\n","color1 = 'tab:red'\n","ax1a.set_xlabel(f'True Anomaly ({nu.unit})')\n","ax1a.set_ylabel(f'Projected Separation ({s.unit})', color=color1)\n","ax1a.plot(nu, s, color=color1)\n","ax1a.tick_params(axis='y', labelcolor=color1)\n","ax1a.set_xlim([0,2*np.pi])\n","\n","ax1b = ax1a.twinx()\n","color2 = 'tab:blue'\n","ax1b.set_ylabel('$\\Delta$mag', color=color2)\n","ax1b.plot(nu, dMag, color=color2)\n","ax1b.tick_params(axis='y', labelcolor=color2)"]},{"cell_type":"markdown","id":"dcc41542-9809-4cf5-a249-656d25f09b0a","metadata":{"id":"dcc41542-9809-4cf5-a249-656d25f09b0a"},"source":["Remember: the point of computing these values is to check whether or not a planet is visible at a given time.  The simplest way to do so is to define limits on observable projected separations and $\\Delta\\mathrm{mag}$ values.  For projected separation, this actually maps quite well to how starlight-suppression systems work.  Essentially all such systems have an inner working angle (IWA) and outer working angle (OWA).  If expressed in units of arcseconds, these values map directly to minimum and maximum observable projected separations (in AU) when multiplied by the distance to the target (in parsecs).  For $\\Delta\\mathrm{mag}$, as we will presently see, setting limits is a bit more complicated, so for now, we will just say that we can pick a constant $\\Delta\\mathrm{mag}$ limit.\n","\n","Let's define some limits and see how these limits interact with our plot:"]},{"cell_type":"code","execution_count":null,"id":"e37985cd-7771-4930-a635-c5cad3c52df1","metadata":{"id":"e37985cd-7771-4930-a635-c5cad3c52df1"},"outputs":[],"source":["IWA = 0.075 #arcseconds\n","OWA = 2 #arcseconds\n","d = 10  #distane to star in parsecs\n","dMaglim = 25 #we have a constant limiting delta magnitude"]},{"cell_type":"markdown","id":"8f5c77e5-8e04-4842-b93a-a2f19d973a6d","metadata":{"id":"8f5c77e5-8e04-4842-b93a-a2f19d973a6d"},"source":["## Exercise 2\n","Using the previously computed values of $s$ and $\\Delta\\mathrm{mag}$, along with the limits defined in the previous block, compute a boolean array of values (of the same size as `s` and `dMag`) that is True where the planet is observable and false otherwise:"]},{"cell_type":"code","execution_count":null,"id":"c1b1923f-f46f-48ae-a862-992a8affc655","metadata":{"id":"c1b1923f-f46f-48ae-a862-992a8affc655"},"outputs":[],"source":["# You are being asked to compute boolean values on an array, which can be done\n","# with comparison operators such as >, >=, ==, etc.  Arrays of boolean values\n","# can also be ANDed or ORed as (bool arr 1) & (bool arr 2) and\n","# (bool arr 1) | (bool arr 2), respectively\n","\n","observable_indices =\n","\n","# if you get stuck, a reference solution is provided by:\n","# observable_indices = SSWYieldModelingTutorial.observable_indices(s, dMag, d, IWA, OWA, dMaglim)"]},{"cell_type":"code","source":["# DELETEME\n","observable_indices = SSWYieldModelingTutorial.observable_indices(s, dMag, d, IWA, OWA, dMaglim)"],"metadata":{"id":"R7sua7c9m9_T"},"id":"R7sua7c9m9_T","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"b7dc1222-59d9-479d-bd74-d8cdd73b214d","metadata":{"id":"b7dc1222-59d9-479d-bd74-d8cdd73b214d"},"outputs":[],"source":["# let's update our plot with the observable regions highlighted\n","# the planet is obserable under our stated constraints (dashed lines)\n","# on the thicker portions of the plots\n","\n","# this is just used to split the observable indices into contiguous blocks for plotting:\n","sepinds = SSWYieldModelingTutorial.split_observable_inds(observable_indices)\n","\n","for inds in sepinds:\n","    ax1a.plot(nu[inds], s[inds],color=color1, linewidth=4)\n","ax1a.plot([0,2*np.pi], [IWA*d]*2, '--', color=color1)\n","\n","for inds in sepinds:\n","    ax1b.plot(nu[inds], dMag[inds],color=color2, linewidth=4)\n","ax1b.plot([0,2*np.pi], [dMaglim]*2, '--', color=color2)\n","\n","# scroll up to see the updates on your original plot"]},{"cell_type":"markdown","id":"10cfdde8-9fff-4dcd-bf42-ccfb882a1656","metadata":{"id":"10cfdde8-9fff-4dcd-bf42-ccfb882a1656"},"source":["## The Impact of Orbital Inclination\n","\n","Of all of the input parameters, the one that has the largest impact on these values is the inclination (why?).  Let's see how the curves change as we change the orbital inclination. Note that this is also a chance to check your results from the first two exercises."]},{"cell_type":"code","execution_count":null,"id":"77bbc150-3322-4295-8fd8-198ec675a9cc","metadata":{"id":"77bbc150-3322-4295-8fd8-198ec675a9cc"},"outputs":[],"source":["# We're going to now compute s and dMag on a grid of I and nu values\n","I0 = np.linspace(0,np.pi,101)*u.rad # full range of possible integrations\n","nu0 = np.linspace(0,2*np.pi,100)*u.rad # The true anomaly varies between 0 and 2\\pi over the course of the orbit\n","[Is, nus] = np.meshgrid(I0,nu0)\n","\n","ss, dMags = SSWYieldModelingTutorial.calc_s_dMag(1*u.AU, 0.2, Is, 15*u.deg,  1*u.earthRad, 0.367, nus)\n","ss = ss.transpose()\n","dMags = dMags.transpose()\n","\n","# same constraints as before:\n","IWA = 0.075 #arcseconds\n","OWA = 2 #arcseconds\n","d = 10  #distane to star in parsecs\n","dMaglim = 25 #we have a constant limiting delta magnitude\n","\n","fig2, ax2a = plt.subplots()\n","\n","color1 = 'tab:red'\n","ax2a.set_xlabel(f'True Anomaly ({nu0.unit})')\n","ax2a.set_ylabel(f'Projected Separation ({ss.unit})', color=color1)\n","scurve = ax2a.plot(nu0, ss[0], color=color1)[0]\n","ax2a.tick_params(axis='y', labelcolor=color1)\n","ax2a.set_xlim([0,2*np.pi])\n","ax2a.plot([0,2*np.pi], [IWA*d]*2, '--', color=color1)\n","\n","ax2b = ax2a.twinx()\n","color2 = 'tab:blue'\n","ax2b.set_ylabel('$\\Delta$mag', color=color2)\n","dMagcurve = ax2b.plot(nu0, dMags[0], color=color2)[0]\n","ax2b.tick_params(axis='y', labelcolor=color2)\n","ax2b.plot([0,2*np.pi], [dMaglim]*2, '--', color=color2)\n","\n","fig2.suptitle(f\"I = {I0[0].to(u.deg) :.2f}\")\n","overlines = []\n","\n","def drawInclinationFrame(j):\n","    scurve.set_data(nu0, ss[j])\n","    dMagcurve.set_data(nu0, dMags[j])\n","    fig2.suptitle(f\"I = {I0[j].to(u.deg):.2f}\")\n","    ax2a.set_ylim([ss[j].value.min(), ss[j].value.max()])\n","    ax2b.set_ylim([dMags[j].min(), dMags[j].max()])\n","    obsinds = SSWYieldModelingTutorial.observable_indices(ss[j], dMags[j], d, IWA, OWA, dMaglim)\n","    sepinds = SSWYieldModelingTutorial.split_observable_inds(obsinds)\n","    while len(overlines) > 0:\n","        overlines.pop().remove()\n","    for inds in sepinds:\n","        overlines.append(ax2a.plot(nu0[inds], ss[j][inds],color=color1, linewidth=4)[0])\n","    for inds in sepinds:\n","        overlines.append(ax2b.plot(nu0[inds], dMags[j][inds],color=color2, linewidth=4)[0])\n","\n","widgets.interact(drawInclinationFrame, j=widgets.IntSlider(min=0, max=len(I0)-1, step=1, value=50));\n","# use slider to change the orbit inclination"]},{"cell_type":"markdown","id":"149095ac-c0d1-44d2-9ae9-af7bf3cfd2f5","metadata":{"id":"149095ac-c0d1-44d2-9ae9-af7bf3cfd2f5"},"source":["A key aspect of what we're seeing here is that orbital inclination places a limit on the range of observable face angle.  A face-on orbit (one with a 0$^\\circ$ inclination) will always be viewed at a 90$^\\circ$ phase (known as **quadrature**). Only an edge-on orbit (one with 90$^\\circ$) inclination will actually pass through the full range of $\\beta$ values (0 to 180$^\\circ$).  For intermediate values of inclination, the planet phase will always be in the range $[90^\\circ - I, 90^\\circ + I]$.\n","\n","Another very important thing to note is the range of projected separations.  If you look back to the orbital radius magnitude equation, you'll note that the orbital radius ranges between $a(1-e)$ and $a(1+e)$.  The projected separation will always be less than or equal to the orbital radius (we can see this by inspection of the equation we derived, above). This means that when the projected separation is smaller than the semi-major axis, this is due to either the orbital eccentricity or projection effects (e.g., when the orbit is anything other than face-on).  When the projected separation is greater than the semi-major axis, this is due *only* to eccentricity.  A cool result that you can prove from all of this is that, given a single observation of a planet, the observed projected separation is actually the maximum likelihood estimate of its semi-major axis."]},{"cell_type":"markdown","id":"77b58251-fa76-4310-914f-bb7d1dc76ffc","metadata":{"id":"77b58251-fa76-4310-914f-bb7d1dc76ffc"},"source":["# The Direct Imaging Observable Joint Probability Density Function\n","\n","Now that we have the ability to compute the imaging observables, we can proceed to compute the joint probability density function of these values for a given population of planets. We will call this density function $f_{\\Delta\\mathrm{mag},s}$.\n","\n","There are multiple different ways to do this calculation, but the conceptually simplest is via Monte Carlo: we will draw random samples from density functions describing all of our input parameters, and then use these to compute the corresponding projected separation and $\\Delta\\mathrm{mag}$ values.  We will then form a 2D histogram over these computed values, thereby estimating their joint probability density function.\n","\n","An important thing to note is that, in reality, the input parameters may not be independent, and may have joint prior functions of their own.  Here, we will ignore this bit of complexity, and treat all inputs as fully independent of one another.\n","\n","We are going to try modeling a population of Earth-like planets.  Note that there is an enormous body of literature establishing various priors for this population. Here, we will make multiple simplifying assumptions.\n","\n","For the semi-major axis, we will assume a uniform distribution between 0.7 and 1.5 AU (note that this assumption is non-physical - real planet orbits appear to be distributed logarithmically in semi-major axis, but this is a reasonable approximation to make when focusing on a small interval of semi-major axes. We will similarly approximate eccentricities as uniformly distributed between 0 and 0.35 (again, this is not what the actual universe appears to do). We will take all of the planets in our population to have radii of exactly 1 Earth radius, and geometric albedos of 0.367.\n","\n","Finally, we will assume that orbit orientation are isotropically distributed over the unit sphere (e.g., that there is no preferred orbital plane).  This means that both $\\omega$ and $\\Omega$ are uniformly distributed between 0 and $2\\pi$ radians, whereas $I$ is sinusoidally distributed, e.g.:\n","$$f_\\bar{I}(I) = \\begin{cases} \\dfrac{\\sin(I)}{2}  & I \\in [0, \\pi] \\\\ 0 & \\mathrm{else} \\end{cases}$$\n","\n","Finally, we are interested in sampling the populations consistently with arbitrarily timed observations.  Thus, we treat the time of observation as uniformly distributed, which corresponds to a uniform distribution in mean anomaly ($M$). This is related to the true anomaly via the Kepler time equation:\n","$$M = E - e\\sin(E)$$\n","where $E$ is the eccentric anomaly, which maps to true anomaly as:\n","$$\\tan\\left(\\frac{E}{2}\\right) = \\sqrt{\\frac{1-e}{1+e}}\\tan\\left(\\frac{\\nu}{2}\\right)$$\n","\n","## Exercise 3\n","Let's generate some values and see how this all works:"]},{"cell_type":"code","execution_count":null,"id":"7b672a58-3621-45f0-8aa2-36e763eef606","metadata":{"id":"7b672a58-3621-45f0-8aa2-36e763eef606"},"outputs":[],"source":["N = int(1e5); #number of samples to generate\n","# these values are constant\n","R_P = (1*u.earthRad).to(u.AU).value # Earth radius-equivalent planet in AU\n","p = 0.367 # geometric albedo\n","\n","# we'll skip the units this time to make computations a bit more efficient\n","# Hint 1: numpy provides the very useful method np.random.uniform. See:\n","# https://numpy.org/doc/stable/reference/random/generated/numpy.random.uniform.html\n","#\n","# Hint 2: A sinusoidal distribution can be generated by taking the\n","#         arccosine of a uniform distribution between -1 and 1\n","\n","# generate an array of semi-major axis values\n","# uniformly distributed between 0.7 and 1.5 AU:\n","avals =\n","\n","# generate an array of eccentricity values\n","# uniformly distributed between 0 and 0.35\n","evals =\n","\n","# generate an array of argument of periapsis values\n","# uniformly distributed between 0 and 2\\pi\n","wvals =\n","\n","# generate an array of inclination values\n","# sinusoidally distributed between 0 and \\pi\n","Ivals =\n","\n","# Generate an array of mean anomaly values\n","# uinformly distributed between 0 and 2\\pi\n","Mvals =\n","\n","# if you get stuck, a reference solution is provided by:\n","#avals, evals, wvals, Ivals, Mvals, R_P, p = SSWYieldModelingTutorial.gen_Earthlike_values(int(1e5))"]},{"cell_type":"code","source":["# DELETEME\n","avals, evals, wvals, Ivals, Mvals, R_P, p = SSWYieldModelingTutorial.gen_Earthlike_values(int(1e5))"],"metadata":{"id":"vkFTOLBBxGRN"},"id":"vkFTOLBBxGRN","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"454fdc12-7159-49ed-95ae-48b30a995ad9","metadata":{"id":"454fdc12-7159-49ed-95ae-48b30a995ad9"},"source":["Now that we have the values, we need a bit more work before we can compute our imaging observables.  In particular, note that the Kepler time equation is a transcendental function, and must be inverted numerically (typically using Newton-Raphson iteration). There are many, many software packages in the Python ecosystem that will do this for you - we will use our own implementation here, but lots of others are available to you.\n","\n","Once we have the true anomaly values, we can use our previously written code to compute the corresponding projected separation and $\\Delta\\mathrm{mag}$ values.  We then use some helper method to compute the 2D histogram of these values (normalized by the number of samples and bin areas to make it representative of a true probability density function) and finally to plot the results."]},{"cell_type":"code","execution_count":null,"id":"610f7025-4283-47a7-86ec-1b753c10c2e8","metadata":{"id":"610f7025-4283-47a7-86ec-1b753c10c2e8"},"outputs":[],"source":["# compute true anomaly from mean anomaly and eccentricity\n","_,_,nuvals = SSWYieldModelingTutorial.invKepler(Mvals, evals, return_nu=True)\n","\n","\n","# set semi-major axis and eccentricity ranges:\n","arange, erange = [0.7, 1.5], [0, 0.35]\n","\n","# compute the imaging observables\n","svals, dMagvals = SSWYieldModelingTutorial.calc_s_dMag(avals, evals, Ivals, wvals,  R_P, p, nuvals)\n","\n","# generate the 2D histogram of the values\n","Cpdf, sax, dMagax = SSWYieldModelingTutorial.gen_Cpdf(1000, dMagvals, svals, arange, erange, p, R_P)"]},{"cell_type":"code","execution_count":null,"id":"2b0365e9-0e9a-446e-82fe-89b8333f4ad6","metadata":{"id":"2b0365e9-0e9a-446e-82fe-89b8333f4ad6"},"outputs":[],"source":["fig3, ax3 = plt.subplots()\n","with warnings.catch_warnings():\n","    warnings.simplefilter(\"ignore\")\n","    cs = ax3.contourf(\n","        sax,\n","        dMagax,\n","        Cpdf,\n","        locator=ticker.LogLocator(),\n","    )\n","ax3.set_xlabel(\"Projected Separation (AU)\")\n","ax3.set_ylabel(\"$\\Delta$mag\");\n","cbar = fig3.colorbar(cs)\n","cbar.ax.set_title(\"$f_{\\Delta\\\\mathrm{mag},s}$\");"]},{"cell_type":"markdown","id":"66906281-814b-4143-aa28-0d18b0040a9c","metadata":{"id":"66906281-814b-4143-aa28-0d18b0040a9c"},"source":["This plot looks a bit sparse, but its hard to tell.  Wouldn't it be great if we could compute the bounds of this density function?  Turns out that, while computing the distribution itself analytically is non-trivial (although possible), it is relatively straightforward to establish upper and lower bounds on $\\Delta\\mathrm{mag}$ as a function of $s$:"]},{"cell_type":"code","execution_count":null,"id":"4f5359df-56fe-4cf0-98f5-4685e41aefdb","metadata":{"id":"4f5359df-56fe-4cf0-98f5-4685e41aefdb"},"outputs":[],"source":["dmagmax, dmagmin, dmag90 = SSWYieldModelingTutorial.calc_Cpdf_limits(sax, arange, erange, p, R_P)\n","ax3.plot(sax, dmagmax, \"r\", linewidth=4, label=\"$\\\\Delta\\\\mathrm{mag}_\\\\mathrm{max}$\")\n","ax3.plot(sax, dmagmin, \"b\", linewidth=4, label=\"$\\\\Delta\\\\mathrm{mag}_\\\\mathrm{min}$\")\n","ax3.plot(sax, dmag90, \"k--\", label=\"$\\\\beta = 90^\\\\circ$\")\n","ax3.legend()\n","ax3.set_ylim([20,40]);\n","# check back to your original plot for the limits"]},{"cell_type":"markdown","id":"a13cbbd7-754c-4bec-97a4-230bcdea6f48","metadata":{"id":"a13cbbd7-754c-4bec-97a4-230bcdea6f48"},"source":["## Fully Sampling the Joint Probability Density Function\n","\n","Clearly, we are failing to completely fill the space with our sampling.  This is one of the drawbacks of any Monte Carlo approach, and is related to the so-called 'curse of dimensionality': the larger the number of dimensions in your system, the more samples you need to fully sample the phase space, quickly leading to an infeasible required number of samples.  On thing helps us in this particular case: if you look carefully, you'll note that the highest probability portion of the space lies along the minimum $\\Delta\\mathrm{mag}$ curve. This means that as we add more samples, we'll automatically fill in the most interesting (to us) part of the phase space first. However, the $10^5$ samples we generated here are clearly inadequate.  Typically, we will draw at least $10^8$ samples for this distribution.  This takes a while, so we've precalculated them for you (and actually gone a bit overboard and generated a full billion samples).  Here's what a better sampled version of this density function looks like (and this is a chance to check your work):"]},{"cell_type":"code","execution_count":null,"id":"fe68c43e-95f4-4b1c-bada-1845a7e4f0b6","metadata":{"id":"fe68c43e-95f4-4b1c-bada-1845a7e4f0b6"},"outputs":[],"source":["# set semi-major axis and eccentricity ranges:\n","arange, erange = [0.7, 1.5], [0, 0.35]\n","R_P = (1*u.earthRad).to(u.AU).value # Earth radius-equivalent planet in AU\n","p = 0.367 # geometric albedo\n","Cpdf, sax, dMagax = SSWYieldModelingTutorial.load_precomputed_completeness()\n","dmagmax, dmagmin, dmag90 = SSWYieldModelingTutorial.calc_Cpdf_limits(sax, arange, erange, p, R_P)\n","fig4, ax4 = plt.subplots()\n","with warnings.catch_warnings():\n","    warnings.simplefilter(\"ignore\")\n","    cs = ax4.contourf(\n","        sax,\n","        dMagax,\n","        Cpdf,\n","        locator=ticker.LogLocator(),\n","    )\n","ax4.set_xlabel(\"Projected Separation (AU)\")\n","ax4.set_ylabel(\"$\\Delta$mag\");\n","cbar = fig4.colorbar(cs)\n","cbar.ax.set_title(\"$f_{\\Delta\\\\mathrm{mag},s}$\");\n","ax4.plot(sax, dmagmax, \"r\", linewidth=4, label=\"$\\\\Delta\\\\mathrm{mag}_\\\\mathrm{max}$\")\n","ax4.plot(sax, dmagmin, \"b\", linewidth=4, label=\"$\\\\Delta\\\\mathrm{mag}_\\\\mathrm{min}$\")\n","ax4.plot(sax, dmag90, \"k--\", label=\"$\\\\beta = 90^\\\\circ$\")\n","ax4.legend()\n","ax4.set_ylim([20,45]);"]},{"cell_type":"markdown","id":"5b7fa20a-f7bc-4fa1-99b1-54c32752d460","metadata":{"id":"5b7fa20a-f7bc-4fa1-99b1-54c32752d460"},"source":["We immediately see that with a full $10^9$ samples, we fully fill the space between our maximum and minimum bounding lines.  More importantly, there are no samples outside of these lines, proving that the bounds are correctly computed. Take a moment to study this plot.  This is often referred to as a 'bird plot' (after the shape), and was first published by Bob Brown in his 2005 paper (possibly the most important contribution to the field of exoplanet yield modeling).  The shape of the joint density function is entirely driven by the input parameters.  The divot on the left-hand side of the plot is due to our inclusion of a maximum eccentricity value (allowing eccentricities up to the upper bound of 1 would fill in the blank area present in the current plot).  Finally, we can confirm what we suspected from our previous iteration of this plot: the highest probability region of this density function lies along the bounding minimum $\\Delta\\mathrm{mag}$ curve."]},{"cell_type":"markdown","id":"fe4e3564-34a1-4175-bda7-2252ca42f6fd","metadata":{"id":"fe4e3564-34a1-4175-bda7-2252ca42f6fd"},"source":["# Completeness\n","\n","We now have all of the building blocks we need in order to compute **completeness**: the probability of detecting a planet from a given population about a given target star with a given instrument, assuming that one is there.  Obviously, this is a whole lot of 'givens', but we've actually already established all of them.  Our assumed population of planets is fully encoded in the joint probability density function we evaluated in the previous section.  The instrument is encoded by the inner and outer working angles and the limiting $\\Delta\\mathrm{mag}$. Putting these two together, the completeness is then just the marginalization of the joint density function under the instrumental constraints:\n","$$ c = \\int_{s_\\textrm{min}}^{s_\\textrm{max}} \\int_0^{\\Delta\\mathrm{mag}_\\mathrm{lim}} f_{\\Delta\\mathrm{mag},s} \\, \\mathrm{d}\\Delta\\mathrm{mag}\\,\\mathrm{d}s$$\n","where $s_\\textrm{min},s_\\textrm{max}$ are the minimum and maximum observable projected separation values, which, as previously discussed can be computed as IWA$d$ and OWA$d$, respectively, where $d$ is the distance to the target star in parsecs.\n","\n","As we have computed our joint observable distribution as a discretized, normalized 2D histogram, we can implement the double integral in the equation above as a double summation (this is equivalent to implementing a Riemann sum):\n","$$ c \\approx \\sum_{s = s_\\textrm{min}}^{s_\\textrm{max}} \\sum_{\\Delta\\mathrm{mag} = 0}^{\\Delta\\mathrm{mag}_\\mathrm{lim}} C_\\mathrm{pdf}(s, \\Delta\\mathrm{mag})$$\n","\n","Essentially, we are carving out a rectangular area within our joint density function (under the assumption that the limiting $\\Delta\\mathrm{mag}$ is constant at all projected separations), and then summing the contents of our histogram within this box.  This looks something like:"]},{"cell_type":"code","execution_count":null,"id":"635ee97f-6433-4949-ab21-0fd6ec4cd60c","metadata":{"id":"635ee97f-6433-4949-ab21-0fd6ec4cd60c"},"outputs":[],"source":["fig4, ax4 = plt.subplots()\n","with warnings.catch_warnings():\n","    warnings.simplefilter(\"ignore\")\n","    cs = ax4.contourf(\n","        sax,\n","        dMagax,\n","        Cpdf,\n","        locator=ticker.LogLocator(),\n","    )\n","ax4.set_xlabel(\"Projected Separation (AU)\")\n","ax4.set_ylabel(\"$\\Delta$mag\");\n","cbar = fig4.colorbar(cs)\n","cbar.ax.set_title(\"$f_{\\Delta\\\\mathrm{mag},s}$\");\n","ylim = [20,45]\n","ax4.set_ylim(ylim);\n","projIWA = 0.075 # AU\n","projOWA = 1 # AU\n","ax4.plot([projIWA] * 2, [ylim[0], 25], \"k--\", label=\"Projected IWA\")\n","ax4.plot([projOWA] * 2, [ylim[0], 25], \"k-.\", label=\"Projected OWA\")\n","ax4.plot([projIWA, projOWA], [25] * 2, \"k:\", label=\"$\\\\Delta$mag=25\")\n","ax4.legend();"]},{"cell_type":"markdown","id":"9dcf73f0-bfff-471b-86cb-949a74c2dfc1","metadata":{"id":"9dcf73f0-bfff-471b-86cb-949a74c2dfc1"},"source":["One caveat here is that the grid that we computed the histogram over may be discretized too coarsely (that is, we may wish to pick limits that fall between grid entries).  We solve this by interpolating over the grid.\n","\n","## Exercise 4\n","\n","Let's try this out.  Write a function that, given the joint observable PDF and instrument limits, computes the completeness."]},{"cell_type":"code","execution_count":null,"id":"9a6d5951-67d0-4432-a3ae-20e59457396c","metadata":{"id":"9a6d5951-67d0-4432-a3ae-20e59457396c"},"outputs":[],"source":["def calc_completeness(Cpdf, sax, dMagax, smin, smax, dMaglim):\n","    \"\"\"Compute the completeness of an observation\n","\n","    Args:\n","        Cpdf (np.ndarray):\n","            2D, normalized, joint probability histogram of s and dMag\n","        sax (np.ndarray):\n","            Projected separation axis of Cpdf (AU)\n","        dMagax (np.ndarray):\n","            dMag axis of Cpdf\n","        smin (arraylike):\n","            Minimum observable projected separation (projected IWA) (AU)\n","        smax (arraylike):\n","            Maximum observable projected separation (projected OWA) (AU)\n","        dMaglim (arraylike):\n","            Maximum observable Delta mag value\n","\n","    Returns:\n","        arraylike:\n","            Completeness values\n","\n","    Notes:\n","        All arraylike inputs must have the same dimensionalities (or be scalars)\n","\n","    \"\"\"\n","\n","\n","    # Your code goes here\n","    # Hint 1: You may find the method scipy.interpolate.RectBivariateSpline useful here. See:\n","    # https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.RectBivariateSpline.html\n","    #\n","    # Hint 2: The contents of Cpdf are probably transposed from what you might expect\n","\n","    comp =\n","\n","    return comp\n","\n","# if you get stuck: a reference implementation is available in SSWYieldModelingTutorial.calc_completeness\n","\n","# when you're done, test your function:\n","calc_completeness(Cpdf, sax, dMagax, 0.075, 1, 25)\n","# the expected output is ~0.42"]},{"cell_type":"markdown","id":"3ade7aa7-ce97-4655-bad7-2c48058bab2a","metadata":{"id":"3ade7aa7-ce97-4655-bad7-2c48058bab2a"},"source":["## The Impact of Stellar Distance\n","Now that we have the capability to compute completeness values, we can explore how they change with inputs.  We've already established that the minimum and maximum observable projected separations are linear functions of stellar distance.  Let's look at how the target star's distance impacts the completeness:"]},{"cell_type":"code","execution_count":null,"id":"5da10b55-f6cb-4003-bf0c-faeadb37a304","metadata":{"id":"5da10b55-f6cb-4003-bf0c-faeadb37a304"},"outputs":[],"source":["# We'll consider targets in the range of 1 to 41 parsecs\n","dists = np.arange(1, 41) # parsecs\n","IWA = 0.05 # arcseconds\n","OWA = 0.25 # arcseconds\n","smins = IWA*dists\n","smaxs = OWA*dists\n","dMaglim = 25\n","comps = SSWYieldModelingTutorial.calc_completeness(Cpdf, sax, dMagax, smins, smaxs, dMaglim)\n","\n","ylim = [20,45]\n","fig5, axs5 = plt.subplots(1, 2)\n","with warnings.catch_warnings():\n","    warnings.simplefilter(\"ignore\")\n","    cs = axs5[0].contourf(\n","        sax,\n","        dMagax,\n","        Cpdf,\n","        locator=ticker.LogLocator(),\n","    )\n","axs5[0].set_xlabel(\"Projected Separation (AU)\")\n","axs5[0].set_ylabel(\"$\\Delta$mag\")\n","pIWA = axs5[0].plot([smins[0]] * 2, [ylim[0], dMaglim], \"k--\", label=\"Projected IWA\")[0]\n","pOWA = axs5[0].plot([smaxs[0]] * 2, [ylim[0], dMaglim], \"k-.\", label=\"Projected OWA\")[0]\n","dmaglim = axs5[0].plot(\n","    [smins[0], smaxs[0]], [25] * 2, \"k:\", label=f\"$\\\\Delta$mag={dMaglim}\"\n",")[0]\n","axs5[0].legend()\n","axs5[0].set_ylim(ylim)\n","xlim0 = axs5[0].get_xlim()\n","fig5.suptitle(f\"$d = ${dists[0]}\")\n","\n","compline = axs5[1].plot(dists[0], comps[0])[0]\n","comppoint = axs5[1].scatter(dists[0], comps[0])\n","axs5[1].set_xlim([0, 40])\n","axs5[1].set_ylim([0, 1])\n","axs5[1].set_xlabel(\"Star Distance (pc)\")\n","axs5[1].set_ylabel(\"Completeness\")\n","\n","plt.subplots_adjust(top=0.925, left=0.075, right=0.975)\n","\n","def drawCompFrame(j):\n","    pIWA.set_data([smins[j]] * 2, [ylim[0], 26])\n","    pOWA.set_data([smaxs[j]] * 2, [ylim[0], 26])\n","    dmaglim.set_data([smins[j], smaxs[j]], [26] * 2)\n","    if smaxs[j] > sax.max():\n","        axs5[0].set_xlim([0, smaxs[j]])\n","    else:\n","        axs5[0].set_xlim(xlim0)\n","    fig5.suptitle(f\"$d = ${dists[j]}\")\n","    compline.set_data(dists[: j + 1], comps[: j + 1])\n","    comppoint.set_offsets((dists[j], comps[j]))\n","\n","widgets.interact(drawCompFrame, j=widgets.IntSlider(min=0, max=len(dists)-1, step=1, value=0));\n","# use slider to change the star distance"]},{"cell_type":"markdown","id":"e1f9ba6a-222c-4e0e-93b0-f384d520a01f","metadata":{"id":"e1f9ba6a-222c-4e0e-93b0-f384d520a01f"},"source":["We can see that the effect of moving to further and further targets: the observable box scales linearly outwards from the star, covering a different subsection of the joint density function for different stellar distances.  For the specific IWA/OWA values we have selected here, the bounding box covers the largest portion of the highest-probability portion of the distribution for a stellar distance of ~7 pc, leading to the highest completeness value at that distance.  \n","\n","**Be very careful here**.  Note *all* of the assumptions that went into this result.  We had to both postulate the underlying planet population *and* the instrument's capabilities. Making changes to either would lead to a different result.\n","\n","The second thing to note is that we only needed to compute the underlying density function once.  In order to get different completeness results for different observing cases, we only adjusted the limits of our integration (summation).  This will be a recurring theme as we study other effects.\n","\n","## Habitable Zones and Stellar Luminosity\n","\n","At first glance, our equations do not predict any changes in completeness with stellar luminosity, but this changes when you focus specifically on Earth-like planets.  While there is not broad consensus on what exactly makes a planet 'Earth-like', in general, you want these planets to receive about as much radiant flux from their host stars as the Earth receives from the sun. This means that we wish to match insolation distance rather than absolute distance between planet and star. That is, we want to scale our planets' semi-major axes as $a = \\sqrt{L}a_{L=1 L_\\odot}$ where $a_{L=1 L_\\odot}$ is the semi-major axis at 1 solar luminosity. The full range of semi-major axes we wish to study is similarly scaled, leading to a very basic definition of the **habitable zone**.\n","\n","This semi-major axis scaling implies that the orbital radius similarly scales as $r = \\sqrt{L}r_{L=1 L_\\odot}$, meaning that so does the projected separation: $$s = \\sqrt{L}s_{L=1 L_\\odot}$$\n","\n","If you plug these scalings into the $\\Delta\\mathrm{mag}$ expression, you find:\n","$$\\Delta\\mathrm{mag} = \\Delta\\mathrm{mag}_{L=1 L_\\odot} + 2.5\\log_{10}\\left(L\\right)$$\n","\n","Note that there's no need to recompute our underlying joint probability density function for different values of stellar luminosity.  Instead, we only need to re-interpret the axes.  With the inclusion of stellar luminosity, we read the abscissa as $s/\\sqrt{L}$ and the ordinate as $\\Delta\\mathrm{mag} - 2.5\\log_{10}\\left(L\\right)$.\n","\n","## Exercise 5\n","\n","Let's try this out. Update your previous completeness calculation function, taking into account the stellar luminosity (under the assumption that we're matching stellar insolation for our planet population)."]},{"cell_type":"code","execution_count":null,"id":"5b7ed5f3-9f4b-4ef4-9852-ba24f59a26d3","metadata":{"id":"5b7ed5f3-9f4b-4ef4-9852-ba24f59a26d3"},"outputs":[],"source":["def calc_completeness(Cpdf, sax, dMagax, smin, smax, dMaglim, L):\n","    \"\"\"Compute the completeness of an observation\n","\n","    Args:\n","        Cpdf (np.ndarray):\n","            2D, normalized, joint probability histogram of s and dMag\n","        sax (np.ndarray):\n","            Projected separation axis of Cpdf (AU)\n","        dMagax (np.ndarray):\n","            dMag axis of Cpdf\n","        smin (arraylike):\n","            Minimum observable projected separation (projected IWA) (AU)\n","        smax (arraylike):\n","            Maximum observable projected separation (projected OWA) (AU)\n","        dMaglim (arraylike):\n","            Maximum observable Delta mag value\n","        L (arraylike):\n","            Stellar luminosity in solar luminosities\n","\n","    Returns:\n","        arraylike:\n","            Completeness values\n","\n","    Notes:\n","        All arraylike inputs must have the same dimensionalities (or be scalars)\n","\n","    \"\"\"\n","\n","\n","    # Your code goes here\n","\n","    comp =\n","\n","    return comp\n","\n","# if you get stuck: a reference implementation is available in SSWYieldModelingTutorial.calc_completeness\n","\n","# when you're done, test your function:\n","calc_completeness(Cpdf, sax, dMagax, 0.075, 1, 25, 1.5)\n","# the expected output is ~0.258"]},{"cell_type":"markdown","id":"bfbb46b1-69b6-448a-bfa5-9db6432e8276","metadata":{"id":"bfbb46b1-69b6-448a-bfa5-9db6432e8276"},"source":["## The Impact of Stellar Luminosity\n","\n","We can now explore how changing the luminosity impacts completeness:"]},{"cell_type":"code","execution_count":null,"id":"9dddc6e4-a8f5-4620-8d67-9027a13e05fb","metadata":{"id":"9dddc6e4-a8f5-4620-8d67-9027a13e05fb"},"outputs":[],"source":["# assume a fixed star at 10 pc:\n","d = 10 # parsecs\n","IWA = 0.05 # arcseconds\n","OWA = 0.25 # arcseconds\n","smins = IWA*d\n","smaxs = OWA*d\n","dMaglim = 25\n","\n","# generate a logarithmically spaced array of luminosities\n","Ls = np.logspace(np.log10(0.06), np.log10(30), 40)\n","comps2 = SSWYieldModelingTutorial.calc_completeness(Cpdf, sax, dMagax, smins, smaxs, dMaglim, Ls)\n","\n","ylim = [20,45]\n","dmags2 = dMaglim - 2.5 * np.log10(Ls)\n","smins2 = smins/np.sqrt(Ls)\n","smaxs2 = smaxs/np.sqrt(Ls)\n","fig6, axs6 = plt.subplots(1, 2)\n","with warnings.catch_warnings():\n","    warnings.simplefilter(\"ignore\")\n","    cs = axs6[0].contourf(\n","        sax,\n","        dMagax,\n","        Cpdf,\n","        locator=ticker.LogLocator(),\n","    )\n","axs6[0].set_xlabel(\"$s/\\sqrt{L}$ (AU)\")\n","axs6[0].set_ylabel(\"$\\Delta$mag - 2.5$\\log_{10}(L)$\")\n","pIWA2 = axs6[0].plot([smins2[0]] * 2, [ylim[0], dmags2[0]], \"k--\", label=\"Projected IWA\")[0]\n","pOWA2 = axs6[0].plot([smaxs2[0]] * 2, [ylim[0], dmags2[0]], \"k-.\", label=\"Projected OWA\")[0]\n","dmaglim2 = axs6[0].plot(\n","    [smins2[0], smaxs2[0]], [dmags2[0]] * 2, \"k:\", label=\"$\\\\Delta$mag$_\\\\mathrm{lim}$\"\n",")[0]\n","axs6[0].legend(loc=1)\n","axs6[0].set_ylim(ylim)\n","fig6.suptitle(f\"$L = ${Ls[0] :.3f} $L_\\odot$\")\n","\n","compline2 = axs6[1].semilogx(Ls[0], comps2[0])[0]\n","comppoint2 = axs6[1].scatter(Ls[0], comps2[0])\n","axs6[1].set_xlim([np.min(Ls), np.max(Ls)])\n","axs6[1].set_ylim([0, 1])\n","axs6[1].set_xlabel(\"Star Luminosity ($L_\\odot$)\")\n","axs6[1].set_ylabel(\"Completeness\")\n","\n","plt.subplots_adjust(top=0.925, left=0.075, right=0.975)\n","\n","\n","def drawCompFrame2(j):\n","    pIWA2.set_data([smins2[j]] * 2, [ylim[0], dmags2[j]])\n","    pOWA2.set_data([smaxs2[j]] * 2, [ylim[0], dmags2[j]])\n","    dmaglim2.set_data([smins2[j], smaxs2[j]], [dmags2[j]] * 2)\n","    if smaxs2[j] > sax.max():\n","        axs6[0].set_xlim([0, np.ceil(smaxs2[j]*10)/10])\n","    else:\n","        axs6[0].set_xlim([0,sax.max()])\n","    fig6.suptitle(f\"$L = ${Ls[j] :.3f} $L_\\odot$\")\n","    compline2.set_data(Ls[: j + 1], comps2[: j + 1])\n","    comppoint2.set_offsets((Ls[j], comps2[j]))\n","\n","widgets.interact(drawCompFrame2, j=widgets.IntSlider(min=0, max=len(Ls)-1, step=1, value=0));\n","# use slider to change the star luminosity"]},{"cell_type":"markdown","id":"758cf607-7756-4c23-b8ff-c2a2b1270b21","metadata":{"id":"758cf607-7756-4c23-b8ff-c2a2b1270b21"},"source":["Note that the effect of scaling stellar luminosity is a bit more complex than when changing the stellar distance, as the luminosity changes the bounds of the box we're integrating within in both dimensions. For low stellar luminosities, the habitable zone moves inwards, towards the star, making the requirements on IWA very challenging.  For a fixed IWA, as we're assuming here, eventually the full joint distribution moves outside of the bounding box (at somewhere around 0.06 solar luminosities). For the assumed fixed distance of 10 pc (and our IWA/OWA/limiting $\\Delta\\mathrm{mag}$) assumptions), we find a peak in the completeness at around 0.65 solar luminosities.  As we continue to increase the luminosity, the habitable zone keeps moving outwards (which makes for easier IWA requirements) but the planets also get fainter (due to the reciprocal orbital radius magnitude term in the $\\Delta\\mathrm{mag}$ equation), making instrumental limiting $\\Delta\\mathrm{mag}$ requirements more challenging. By the time we get to 30 solar luminosities (roughly an A1V star - think just a bit dimmer than Vega), the bounding box has fully moved *below* the joint density function and the completeness is once again zero.\n","\n","Finally, always remember that both the luminosity scaling and stellar distance scaling effects are happening at the same time!\n","\n","# Integration Time\n","\n","You may have noticed a glaring gap in our formulation of how we compute completeness: it appears to be entirely independent of integration (or exposure) time.  This is decidedly non-physical, as, intuitively, we should be able to see fainter planets if we collect more photons.  In reality, the integration time folds into the determination of our limiting $\\Delta\\mathrm{mag}$.  In order to see how, we first need to define an integration time model.\n","\n","## Signal to Noise Ratio\n","\n","We start by modeling the signal that we collect with our instrument as the sum of the photons from the thing we wish to measure (e.g., a planet, so we'll call this $c_p$) plus the photons from everything else (e.g., the background, so we'll call this $c_b$).  We can equivalently define the rates of photon arrival from each of these sources (which we'll represent with an overbar) such that our total measurement is:\n","$$z = \\bar{c}_pt_\\mathrm{int} +  \\bar{c}_bt_\\mathrm{int}$$\n","\n","We ultimately wish to make a statistical statement about what we thought we detected, which is typically encoded as a **signal to noise** (SNR) metric (or ratio).  SNR actually has *a lot* of formal definitions, which vary by field and application area, but the one most useful in our case is the ratio of the expectation value of the signal (the thing we wish to measure) to the standard deviation of the noise (everything else).  Photon arrival is a Poisson process&mdash;that is, the distribution of photons collected in a given span of time is governed by a Poisson distribution, which describes the probability of recording a certain number of events per unit time when the events have a constant rate of occurrence. One nice attribute of a Poisson distribution is that its standard deviation is equal to the square root of its mean value.\n","\n","So, under all these assumptions, our SNR metric is defined as:\n","$$\\mathrm{SNR} = \\frac{\\bar{c}_pt_\\mathrm{int}}{\\sqrt{ \\bar{c}_pt_\\mathrm{int}  + \\bar{c}_bt_\\mathrm{int} }} = \\frac{\\bar{c}_p}{\\sqrt{ \\bar{c}_p  + \\bar{c}_b }} \\sqrt{t_\\mathrm{int}}$$\n","\n","Note that the denominator (the noise term) includes both the planet and background counts.  This is because Poisson noise (sometimes called **shot noise**) is generated by *all* incoming photons.  One immediate observation is that our SNR goes as the square root of integration time. So, if you want to double your SNR, you need to quadruple how long you collect photons for. The other thing to note is that this expression appears to predict that you can get to an arbitrarily high SNR if you are willing to integrate for an arbitrary long time.  This latter assertion is problematic because it is inherently non-physical for every optical system built to date.  In general, there will always be some **noise floor** (typically linked to the inherent stability of the optical system) that means that at a certain integration time, you stop gaining any SNR. Detailed modeling of such a noise floor can be arbitrarily complex, so instead, we will provide ourselves with a simplified model of the form:\n","$$\\mathrm{SNR} = \\frac{\\bar{c}_pt_\\mathrm{int}}{\\sqrt{ \\bar{c}_pt_\\mathrm{int}  + \\bar{c}_bt_\\mathrm{int} + (Mt_\\mathrm{int})^2) }}$$\n","where $M$ is a term inherently capturing our system's noise floor.  As $Mt_\\mathrm{int}$ is added to our noise term in quadrature, $M$ can be interpreted as the photon rate associated with the standard deviation of a noise component that fundamentally cannot be subtracted out from our measured signal.\n","\n","Using these definitions, we can now invert the expression to compute integration times as:\n","$$t_\\mathrm{int} = \\frac{ \\bar{c}_p + \\bar{c}_b }{\\left(\\dfrac{\\bar{c}_p}{\\mathrm{SNR}}\\right)^2 - M^2}$$\n","which imposes the condition:\n","$$M < \\frac{ \\bar{c}_p}{\\mathrm{SNR}}$$\n","These last two expressions fundamentally express the idea that for a given optical system and planet and noise count rates, there will be signal to noise ratios that are fundamentally unachievable (which matches our actual experience in real life).\n","\n","## Exercise 6\n","\n","Write a function to compute integration times given input values of $ \\bar{c}_p, \\bar{c}_b$, $M$ and the required SNR."]},{"cell_type":"code","execution_count":null,"id":"6c087a6d-192f-49f4-a34d-edac021767da","metadata":{"id":"6c087a6d-192f-49f4-a34d-edac021767da"},"outputs":[],"source":["def calc_intTime(C_p, C_b, M, SNR):\n","    \"\"\"Find the integration time to reach a required SNR given the planet and\n","    background count rates as well as the optical system's noise floor.\n","\n","\n","    Args:\n","        C_p (arraylike Quantity):\n","            Planet count rate (1/time units)\n","        C_b (arraylike Quantity):\n","            Background count rate (1/time units)\n","        M (arraylike Quantity):\n","            Noise floor count rate (1/time units)\n","        SNR (float):\n","            Required signal to noise ratio\n","\n","    Returns:\n","        ~astropy.units.Quantity(~numpy.ndarray(float)):\n","            Integration times\n","\n","    .. note::\n","\n","        All infeasible integration times should be returned as NaN values\n","\n","    \"\"\"\n","\n","    # your code goes here:\n","    intTime =\n","\n","\n","    # infinite and negative values are set to NAN\n","    intTime[np.isinf(intTime) | (intTime.value < 0.0)] = np.nan\n","\n","    return intTime\n","\n","# if you get stuck: a reference implementation is available in SSWYieldModelingTutorial.calc_intTime\n","\n","# when you're done, test your function:\n","C_p = 0.03/u.s # planet count rate photons/second (photons excluded from unit)\n","C_b = 0.01/u.s # background count rate photons/second (photons excluded from unit)\n","M = 0.003/u.s  # noise floor count rate photons/second (photons excluded from unit)\n","SNR = 5\n","calc_intTime(C_p, C_b, M, SNR)\n","# the expected output is ~1481.5 seconds"]},{"cell_type":"markdown","id":"69674f41-2356-4154-8bf9-651baa2f7587","metadata":{"id":"69674f41-2356-4154-8bf9-651baa2f7587"},"source":["## Modeling Planet and Background Counts\n","\n","Now that we have our SNR and integration time equations, we need to be able to model the individual terms involved, and map them to the quantities that determine completeness. We'll start with the planet count rate.\n","\n","### Planet Count Rate\n","Remember that the $\\Delta\\mathrm{mag}$ encodes the flux ratio of the planet to its host star. Therefore, we can express the spectral flux density (or irradiance) of a planet as $C_\\mathrm{star} 10^{-0.4\\Delta\\mathrm{mag}}$ where $C_\\mathrm{star}$ is the spectral flux density of the star, and -0.4 is the reciprocal of -2.5. We can further define:\n","$$C_\\mathrm{star} = \\mathcal{F}_0 10^{-0.4\\mathrm{mag}_\\mathrm{star}}$$\n","where $\\mathcal{F}_0$ is the spectral flux density (in your observing wavelength band) of a zero-magnitude star, and $\\mathrm{mag}_\\mathrm{star}$ is the apparent magnitude of your target star in that same band.\n","\n","We've now defined the spectral flux density of the planet, but we actually care about the planet count rates recorded on our instrument's detector.  Recall that spectral flux density is energy per time per area per wavelength (thanks to the Planck constant, equivalently photons per time per area per wavelength). In order to compute the planet count rate, we need to scale the spectral flux density by our telescope's collecting area, the wavelength range of our observing band (the bandpass), and any losses due to the optical system.  So, we define:\n","$$\\bar{c}_p =  \\mathcal{F}_0 10^{-0.4(\\mathrm{mag}_\\mathrm{star}+ \\Delta\\mathrm{mag})}A \\Delta\\lambda \\tau \\tau_\\mathrm{core} \\mathrm{QE}$$\n","where $\\Delta\\lambda$ is the bandpass of our observing band, $\\tau$ is the throughput of our system due to everything *other* than whatever technology is used to suppress the star's light (that is, all losses due to internal reflections and transmissions), and $\\tau_\\mathrm{core}$ is the throughput of our starlight suppression technology for point sources. $A$ is the effective collecting area of our telescope (e.g. the primary mirror area minus the area of any obscuration), which we compute (for a circular aperture of diameter $D$ and obscuration factor $f_\\mathrm{obsc}$) as:\n","$$A = \\pi \\left(\\frac{D}{2}\\right)^2(1 - f_\\mathrm{obsc})\\,.$$\n","Finally QE is the quantum efficiency of our detector: this is the mean rate at which photons are converted to electrons (which is what ultimately gets measured and recorded). If you're keeping track of units, the application of the QE factor transforms any units of photons to units of electrons.  In real detectors, there's some additional complexity in the digitization of the electron counts to actual recorded counts (typically represented as digital numbers or DN units), but for simplicity we will assume that one electron produces on recorded count value.\n","\n","To make things a bit cleaner, we can collect terms that will scale multiple of our count rates into a single parameter,\n","$\\eta \\triangleq A \\Delta\\lambda \\tau QE$, such that:\n","$$\\bar{c}_p =  \\mathcal{F}_0 10^{-0.4(\\mathrm{mag}_\\mathrm{star}+ \\Delta\\mathrm{mag})}\\eta \\tau_\\mathrm{core}\\,.$$\n","\n","### Astrophysical Background Count Rates\n","The background signal will include counts from astrophysical sources as well as noise from the instrument itself.  The most important astrophysical background sources are the local zodiacal light ($\\bar{c}_\\mathrm{zodi}$; light reflected form the dust cloud shrouding the inner solar system), exozodiacal light ($\\bar{c}_\\mathrm{exozodi}$; light reflected from similar dust clouds in the target exosystem), and residual starlight not blocked by our starlight suppression technology ($\\bar{c}_{sr}$). We model the local zodiacal light contribution as:\n","$$\\bar{c}_\\mathrm{zodi} = \\mathcal{F}_0 10^{-0.4z} \\Omega A \\Delta\\lambda \\tau \\tau_\\mathrm{occ} \\mathrm{QE} = \\mathcal{F}_0 10^{-0.4z} \\Omega\\eta\\tau_\\mathrm{occ}$$\n","where $\\tau_\\mathrm{occ}$ is the throughput of our starlight suppression technology for infinitely extended sources, $z$ is the surface brightness of the local zodi in units of magnitudes per unit solid angle (nominally 23 mag arcsec$^{-2}$), and $\\Omega$ is the solid angle of our photometric aperture. We will assume that our instrument is critically sampled at the central wavelength of our bandpass ($\\lambda$) then we can compute the photometric aperture solid angle as:\n","$$\\Omega = \\pi\\left(\\frac{\\lambda}{2D}\\right)^2 \\,.$$\n","Note that the unit of this value (if the wavelength and primary diameter have the same length unit) is a steradian (radian^2).  To make this work with the previous expression, we need to convert this to square arcseconds.\n","\n","The exozodi contribution is computed in the exact same way, except that we replace the value of $z$ with whatever we are assuming for the brightness of the exozodi (at this point, we literally don't know what this should be - we haven't built instruments sensitive enough to directly measure this yet)."]},{"cell_type":"markdown","id":"3d097349-baa3-420e-a062-76699f66e81a","metadata":{"id":"3d097349-baa3-420e-a062-76699f66e81a"},"source":["### Instrumental Background Count Rates\n","\n","Another major source of background signal is due to the planet's host star.  While our starlight suppression technologies are designed to remove light from the star, they cannot do so perfectly, and so there will always be some amount of starlight that makes it into our science images.  We characterize this quantity via our instrument's contrast ($C$): the ratio of off-axis flux (at the location where we're making our measurement) to the peak of the star's flux in a measurement taken *without* the starlight suppression system in place. This value essentially encodes how efficiently we are able to block starlight.  Note that while this value is similar to the $\\Delta\\mathrm{mag}$, it is inherently an instrumental property whereas $\\Delta\\mathrm{mag}$ is an astrophysical one.  While many references may conflate the two, it is very important to understand the difference: $\\Delta\\mathrm{mag}$ is what the universe gives you, while $C$ is what your instrument is designed to do.  Given the instrument's contrast, we can compute the count rate due to the star's residual (unblocked) light as:\n","$$\\bar{c}_{sr} =  C_\\mathrm{star} C A \\Delta\\lambda \\tau \\tau_\\mathrm{core} \\mathrm{QE} = C_\\mathrm{star} C\\eta \\tau_\\mathrm{core} $$\n","\n","Finally, we have background signal that is generated by the detector itself.  Modeling of detectors can get arbitrarily complicated, as there is a *lot* of very specific (and fascinating) physics in play here.  This is especially true when considering photon-counting detectors (literally devices that can, on average, track every individual photon intercepted).  Here, we will limit ourselves to two instrumental noise signals: dark current and read noise. Dark current is the signal generated by a detector when it is not illuminated, while read noise consists of excess electrons that are generated by the process of reading out the detector (that is, literally collecting all of the light accumulated in the detector and converting it to a digital signal).  Both of these are (on average) per-pixel effects, and so we need to know the relevant number of pixels we should be computing these for.  This requires us to define the number of pixels covered by our photometric aperture:\n","$$N_\\mathrm{pix} = \\frac{\\Omega}{\\theta_\\mathrm{det}^2}$$\n","where $\\theta_\\mathrm{det}$ is the detector's pixel scale (the instantaneous field of view of each detector pixel).\n","The dark current count rate is then:\n","$$\\bar{c}_\\mathrm{dark} = N_\\mathrm{pix} i_\\mathrm{dark}$$\n","where $i_\\mathrm{dark}$ is the dark current rate, measured in counts/second/pixel.\n","\n","All of the preceding count rates have come from processes that are (or can be safely modeled as) continuous.  That is, for our astrophysical sources and starlight residual and dark current, we are accumulating photons (counts or digital numbers) throughout the length of an observation that have some average arrival rate, such that we can multiply these rates by the total integration time to get the expectation value for our recorded signal.  Read noise, however, is different as it represents a *discrete* process.  Counts due to read noise are only generated when we read out our detector.  We could do this once per observation, but that tends to be a bad idea because of the accumulation of cosmic rays, which tend to saturate our detector, as well as some other more finicky effects. So, in typical usage, we will read out our detector multiple times within the span of a single observation, and then sum the results.  To model this, we introduce the exposure time ($t_\\mathrm{exp}$) parameter, which encodes the duration of each readout exposure.  The number of readouts is therefore given by $t_\\mathrm{int}/t_\\mathrm{exp}$. Because we have the integration time in the numerator, we can still compute a 'rate' for the contribution of the read noise, which will have the form:\n","$$\\bar{c}_\\mathrm{read} = N_\\mathrm{pix} \\frac{i_\\mathrm{read}}{t_\\mathrm{exp}}$$\n","where $i_\\mathrm{read}$ is the read noise in units of electrons/pixel/readout."]},{"cell_type":"markdown","id":"95f1f1a1-1d05-4813-b390-e3464e085d4b","metadata":{"id":"95f1f1a1-1d05-4813-b390-e3464e085d4b"},"source":["## Modeling the Noise Floor\n","\n","Perhaps the most difficult aspect of all of this to model is the noise floor ($M$). Conceptually, this term represents the part of the background signal that cannot be subtracted or otherwise removed from our measurements.  This implies that this term has to do with a variability in our optical system that we cannot model.  Formally, we can describe this as a variability in our instrument's **point spread function** (or impulse response).  The actual part of the background signal that will be most affected by such variability is the residual starlight that cannot be full suppressed.  We know, thanks to experience with high-performance, ground-based coronagraphs as well as space-based instrument on the Hubble and other observatories that we are able to subtract the contributions of some, but not all of this term, so we define a new variable, called the post-processing factor ($f_p$) such that:\n","$$M = \\bar{c}_{sr} f_p \\,.$$\n","The post-processing factor should be interpreted as the reciprocal of our expected ability to suppress residual starlight, such that $f_p = 0.1$ represents the assumption that our intrinsic noise floor lies at a factor of 10 below the residual starlight."]},{"cell_type":"markdown","id":"44a81625-3185-40be-b3a7-be2f3b1dcc1b","metadata":{"id":"44a81625-3185-40be-b3a7-be2f3b1dcc1b"},"source":["## Exercise 7\n","\n","We now have all of the equations we need in order to implement a calculation of the planet, background, and noise floor count rates (the inputs to our previously written integration time calculation), so let's do just that.  First, a few implementation notes.  As you can see, there are a *lot* of inputs.  To make our lives a little easier, we will package these into three dictionaries: `static_params` (the values we expect to be more or less constant between different observations), `coronagraph` (values encoding the performance of the starlight suppression system) and `target` (values encoding the particulars of a given observation). Defining these dictionaries serves to prevent errors due to keeping track of the position of a lot of required inputs.\n","\n","<div class=\"alert alert-block alert-danger\">\n","<b>Warning</b> A key thing to keep in mind is that much of the modeling described in the previous section is greatly simplified, and omits a lot of finicky details that become important as you try to compute realistic integration times and count rates. Furthermore, many of the values (such as all of the coronagraph parameters and some of the detector parametrs) are not actually constants, but depend on other parameters such as wavelength, and the separation angles between the star and the planet. Our various simplifications are fine in the context of this tutorial, but be careful in trying to apply any of this in the real world.\n","</div>\n","\n","You'll also note that there are a lot of units floating around these calculations.  We will again make use of `astropy`'s `Unit` class (https://docs.astropy.org/en/stable/units/) to help us, but these still have some limitations.  Most importantly, you cannot use an `astropy` `Quantity` object in an exponent, so we will omit the units on the zodi and exozodi brightness inputs.  Similarly, mathematical operations on quantities with units will work as expected, but may produce results with mixed units (for example nanometers divided by meters).  Every `Quantity` object has a useful built-in method called `decompose()` which will automatically resolve such mixtures to a single base unit.  Similarly, if you ever wish to extract the unit or value of a given quantity object `q`, you can call `q.unit` and `q.value`, respectively.\n","\n","One more important note on units: there are cases where you divide two length units and expect the result to be an angle quantity (for example $\\lambda/D$ is an angle such that if both inputs have the same length unit, the output is in radians).  The `Quantity` objects aren't quite smart enough to figure this out on their own, but allow you to specify those cases where dimensionless quantities should be interpreted as angles, via the `equivalencies` keyword input to the `to` conversion method.  For example:"]},{"cell_type":"code","execution_count":null,"id":"194d1951-b729-4b56-8815-5f07e1847327","metadata":{"id":"194d1951-b729-4b56-8815-5f07e1847327"},"outputs":[],"source":["lam = 500*u.nm # wavelength\n","D = 6*u.m # diameter\n","lD = (lam/D).to(u.arcsec, equivalencies=u.dimensionless_angles()) # lambda/D in angle units\n","print(lD)"]},{"cell_type":"markdown","id":"62650a07-e82f-42ff-b18f-a3cd3441830b","metadata":{"id":"62650a07-e82f-42ff-b18f-a3cd3441830b"},"source":["We will first define some representative input quantities for our count rate function:"]},{"cell_type":"code","execution_count":null,"id":"ef5320c3-e1d8-43bf-a860-cdcb5422c5f8","metadata":{"id":"ef5320c3-e1d8-43bf-a860-cdcb5422c5f8"},"outputs":[],"source":["# define sample inputs to our count rate function\n","static_params = {\"lam\": 550*u.nm, # 550 nm central wavelength\n","                 \"deltaLam\": 110*u.nm, # 20% bandpass\n","                 \"D\": 6*u.m, # 6 meter telescope\n","                 \"obsc\": 0.1, # Primary is 10% obscured\n","                 \"tau\": 0.5, # The non-coronagraphic throughput\n","                 \"QE\": 0.9, # 90% Quantum Efficiency\n","                 # Zero-mag flux in photons cm^-2 nm^-1 s^-1 (photons omitted from unit):\n","                 \"F0\": 12000/u.cm**2/u.nm/u.s, # Zero-magnitude flux (approximate)\n","                 \"pixelScale\": 0.02*u.arcsec, # instantaneous field of view of each detector pixel\n","                 # dark current in counts/second/pixel (counts and pixels ommited from unit):\n","                 \"darkCurrent\": 0.0001/u.s,\n","                 \"readNoise\": 1e-6, # read noise in electrons/pixel/read\n","                 \"texp\": 100*u.s, # single exposure time\n","                 \"ppFac\": 0.1, # post-processing factor\n","                }\n","\n","coronagraph = {\"tau_core\": 0.1, # point source throughput\n","               \"tau_occ\": 0.2,  # extended source throughput\n","               \"contrast\": 4e-11, # contrast (this represents a *very* good coronagraph)\n","              }\n","\n","target = {\"mag_star\": 5, # apparent magnitude\n","          \"zodi\": 23, # local zodi (units omitted as we're using this as an exponent)\n","          \"exozodi\": 22, # exozodi (assume a bit brighter than local zodi)\n","         }"]},{"cell_type":"markdown","id":"c1036e3a-f0b9-4db0-a447-21e04e85756a","metadata":{"id":"c1036e3a-f0b9-4db0-a447-21e04e85756a"},"source":["Now implement your function:"]},{"cell_type":"code","execution_count":null,"id":"88021787-368b-46a4-bc7e-61d96555d010","metadata":{"id":"88021787-368b-46a4-bc7e-61d96555d010"},"outputs":[],"source":["def Cp_Cb_M(static_params, coronagraph, target, deltaMag):\n","    \"\"\"Calculates electron count rates for planet signal, background noise,\n","    and noise floor for an observation.\n","\n","    Args:\n","        static_params (dict):\n","            Dictionary of static parameters:\n","            lam (Quantity):\n","                Central wavelength of observing bandpass (length unit)\n","            deltaLam (Quantity):\n","                Bandpass (length unit)\n","            D (Quantity):\n","                Telescope aperture diameter (length unit)\n","            obsc (float):\n","                Fraction of the primary aperture that is obscured by secondary and\n","                secondary support structures\n","            tau (float):\n","                Optical system throughput excluding effects of starlight suppression\n","                system\n","            QE (float):\n","                Detector quantum efficiency\n","            F0 (Quantity):\n","                Spectral flux density of zero-magnitude star in observing band\n","                (1/length^2/length/time unit)\n","            pixelScale (Quantity):\n","                Instantaneous field of view of each detector pixel (angle unit)\n","            darkCurrent (Quantity):\n","                Dark current in counts/second/pixel (1/time unit)\n","            readNoise (float):\n","                Read noise in electrons/pixel/read\n","            texp (Quantity):\n","                Single readout exposure time (time unit)\n","            ppFac (float):\n","                Post-processing factor\n","\n","        cornagraph (dict):\n","            Dictionary of coronagraph parameters for this observation:\n","            tau_core (float):\n","                Throughput of starlight suppression system for point sources\n","            tau_occ (float):\n","                Throughput of starlight suppression system for infinitely extended\n","                sources\n","\n","        target (dict):\n","            Dictionary of target observing parameters:\n","            mag_star (float):\n","                Apparennt magnitude of target star in observing band\n","            zodi (Quantity)\n","                Surface brightness of local zodi in units of magnitude/arcsec^2.\n","                Input must have units of angle^{-2}.\n","            exozodi (Quantity)\n","                Surface brightness of exozodi in units of magnitude/arcsec^2.\n","                Input must have units of angle^{-2}.\n","        deltaMag (float):\n","                Assumed delta magnitude of planet in observing band\n","\n","\n","    Returns:\n","        tuple:\n","            C_p (~astropy.units.Quantity(~numpy.ndarray(float))):\n","                Planet signal electron count rate in units of 1/s\n","            C_b (~astropy.units.Quantity(~numpy.ndarray(float))):\n","                Background noise electron count rate in units of 1/s\n","            M (~astropy.units.Quantity(~numpy.ndarray(float))):\n","                Residual starlight spatial structure (systematic error)\n","                in units of 1/s\n","    \"\"\"\n","\n","    # Compute telescope collecting area:\n","    # this value should have units of length^2\n","    A =\n","\n","    # compute the common factor of A*tau*deltaLam*QE\n","    # this value should have units of length^3\n","    eta =\n","\n","    # Compute the count rates of the planet\n","    # this should have units of 1/time\n","    C_p =\n","\n","    # Compute size of critically sampled photometric aperture\n","    # This should have units of angle^2\n","    # Hint: this is where that equivalencies input might be handy\n","    Omega =\n","\n","    # Compute the local and exozodi count rates\n","    # this should have units of 1/time\n","    C_zodi =\n","\n","    # Compute the local and exozodi count rates\n","    C_exozodi =\n","\n","    # Compute the count rates of the starlight residual\n","    # this should have units of 1/time\n","    C_sr =\n","\n","    # number of detector pixels in the photometric aperture = Omega / theta^2\n","    # this value should be unitless\n","    Npix =\n","\n","    # Compute the dark current count rate\n","    # this should have units of 1/time\n","    C_dc =\n","\n","    # Compute the read noise count rate\n","    # this should have units of 1/time\n","    C_rn =\n","\n","    # total background signal rate\n","    # this should have units of 1/time\n","    C_b = C_sr + C_zodi + C_exozodi + C_dc + C_rn\n","\n","    # compute the noise floor rate\n","    # this should have units of 1/time\n","    M =\n","\n","    return C_p, C_b, M\n","\n","\n","\n","# if you get stuck: a reference implementation is available in SSWYieldModelingTutorial.Cp_Cb_M"]},{"cell_type":"code","execution_count":null,"id":"15ee3252-ce20-4cc6-a3a9-e4523e43021d","metadata":{"id":"15ee3252-ce20-4cc6-a3a9-e4523e43021d"},"outputs":[],"source":["# Now test your function\n","C_p, C_b, M = Cp_Cb_M(static_params, coronagraph, target, 25)\n","# should be ~ 0.015/s, 0.025/s, 0.0006/s\n","intTime = calc_intTime(C_p, C_b, M, 5)\n","# should be ~ 4560 seconds or ~1.26 hours"]},{"cell_type":"markdown","id":"8bae5aa3-060e-4b3d-8c36-1b7fb7b41ee0","metadata":{"id":"8bae5aa3-060e-4b3d-8c36-1b7fb7b41ee0"},"source":["Note that we now also have the ability to figure out the maximum achievable SNR for a given observation from our previously derived constraint: $$M < \\frac{ \\bar{c}_p}{\\mathrm{SNR}} \\quad \\Longrightarrow \\quad \\mathrm{SNR} < \\frac{ \\bar{c}_p}{M}$$"]},{"cell_type":"code","execution_count":null,"id":"faf46809-b827-44c7-960b-6449425183e8","metadata":{"id":"faf46809-b827-44c7-960b-6449425183e8"},"outputs":[],"source":["maxSNR = C_p/M\n","print(maxSNR) # will be 25"]},{"cell_type":"markdown","id":"61c77948-c223-4212-b6d5-f1a0338e69ff","metadata":{"id":"61c77948-c223-4212-b6d5-f1a0338e69ff"},"source":["## Inverting the Integration Time\n","\n","While the calculation of integration time given an assumed planet $\\Delta\\mathrm{mag}$ is important, it is not the end of the story. That's because we have set up our machinery to compute completeness as a function of limiting $\\Delta\\mathrm{mag}$.  That means that if we wish to calculate the completeness associated with a given integration time, we must be able to invert our integration time calculation to compute the equivalent maximum $\\Delta\\mathrm{mag}$ achievable for some integration time. This actually represents a key decision about how an exoplanet imaging survey is to be conducted. There are basically two options (with many possible minor variations):\n","\n","1. Integrate on every target star to the same $\\Delta\\mathrm{mag}$ - that is, compute the integration time for each target using the same input $\\Delta\\mathrm{mag}$ value to the integration time equation.\n","2. Select integration times on every star to achieve different maximum $\\Delta\\mathrm{mag}$ and optimize the overall observing program to maximize the total accumulated completeness.\n","\n","The two approaches lead to different observing schedules and (likely) different mission outcomes.  While it may initially appear that the latter (optimization) approach is superior, it is important to understand that both methodologies have pros and cons.  The first approach produces a uniform survey and ensures that we can set a uniform bound on the existence of planets for all of the stars we observe. This makes it simpler to make statements about the actual population of planets that are out there once the survey is completed.  Such statements are also possible with the optimization approach, but require a lot more detailed modeling and careful treatment of the survey model that is used for population inference. On the other hand, integrating to a high $\\Delta\\mathrm{mag}$ value on a dim star is wasteful, and that time is likely better spent observing multiple brighter stars, or doing repeated observations on a previously discovered planet.  The completeness maximization approach makes such decisions automated and guarantees a maximized survey completeness (at least on paper).  However, you should never forget that all of the statements being made here are *statistical* in nature.  The maximum summed completeness produced by the numerical optimization represents an expectation value - it is the completeness we would achieve, on average, if we were able to run our experiment (the survey) many times over.  In reality, however, we only get to do the survey once, and it is impossible to detect fractional planets.  It would be a major bummer if we failed to discover a planet because our numerical optimization told us to integrate for a bit shorter on a given star, and the planet was just a bit dimmer than what our resulting observation could detect.  Conversely, the time we save on that integration could allow us to add another target to our list and therefore improve our odds of finding a planet elsewhere.  In short: this is very much an active area of research, and more remains to be said here.\n","\n","For now, let us see how we can invert the integration time equation to do such calculations.  Luckily for us, our expression from above is analytically invertible (which is not necessarily the case for all integration time models).  First, we invert the integration time expression via the quadratic equation:\n","$$t_\\mathrm{int} = \\frac{ \\bar{c}_p + \\bar{c}_b }{\\left(\\dfrac{\\bar{c}_p}{\\mathrm{SNR}}\\right)^2 - M^2} \\quad \\Longrightarrow \\quad \\bar{c}_p = \\frac{\\mathrm{SNR} \\left(\\mathrm{SNR} + \\sqrt{4 M^{2} t_\\mathrm{int}^{2} + 4 \\bar{c}_b t_\\mathrm{int} + \\mathrm{SNR}^{2}}\\right)}{2 t_\\mathrm{int}} \\,.$$\n","Note that we use only the solution that yields a positive value (as negative count rates are non-physical).  We next solve our $\\bar{c}_p$ model for $\\Delta\\mathrm{mag}$:\n","$$\\bar{c}_p = \\mathcal{F}_0 10^{-0.4(\\mathrm{mag}_\\mathrm{star}+ \\Delta\\mathrm{mag})}\\eta\\tau_\\mathrm{core} \\quad \\Longrightarrow \\quad   \\Delta\\mathrm{mag} = -\\mathrm{mag}_\\mathrm{star} -2.5\\log_{10}\\left(\\frac{\\bar{c}_p}{\\mathcal{F}_0\\eta\\tau_\\mathrm{core}}  \\right) \\,.$$\n","\n","Substituting the former expression into the latter gives us a way of computing $\\Delta\\mathrm{mag}$ given an integration time (and therefore the completeness generated by a given integration time). Let's try it out.\n","\n","## Exercise 8\n","Implement a method for computing the largest achievable $\\Delta\\mathrm{mag}$ for a given integration time.  We will use essentially the same inputs as we did for our previous method and will actually call it to compute $\\bar{c}_b$ and $M$.  This is slightly wasteful, as it includes an unnecessary, redundant, calculation of $\\bar{c}_p$, however, this is preferable to copy/pasting large amounts of code."]},{"cell_type":"code","execution_count":null,"id":"04994b85-24d9-4a4f-8451-903eeff4fbb3","metadata":{"id":"04994b85-24d9-4a4f-8451-903eeff4fbb3"},"outputs":[],"source":["def calc_dMag_from_intTime(static_params, coronagraph, target, SNR, intTime):\n","    \"\"\"Find the achievable deltaMag for an observation for given SNR and integration\n","    time.\n","\n","    Args:\n","        static_params (dict):\n","            Dictionary of static parameters. See Cp_Cb_M for details.\n","\n","        cornagraph (dict):\n","            Dictionary of coronagraph parameters for this observation.\n","            See Cp_Cb_M for details.\n","        target (dict):\n","            Dictionary of target observing parameters. See Cp_Cb_M for details.\n","        SNR (float):\n","            Required signal to noise ratio\n","        intTime (Quantity):\n","            Integration time. Units of time.\n","\n","    Returns:\n","        float:\n","            Achievable deltaMag\n","\n","    \"\"\"\n","\n","    # Get values of C_b and M (first output is not needed)\n","    # Note that the deltaMag input here is entirely arbitrary, as we're not\n","    # using the C_p output.\n","    _, C_b, M = Cp_Cb_M(static_params, coronagraph, target, 0)\n","\n","    # Compute C_p\n","    # This should have units of 1/time\n","    C_p =\n","\n","    # compute the common factor of A*tau*deltaLam*QE\n","    # this value should have units of length^3\n","    # Hint: reuse your code from the last exercise\n","    eta =\n","\n","    # Compute the deltaMag corresponding to the inputs\n","    # This should be unitless\n","    deltaMag =\n","\n","    # return just the value\n","    return deltaMag.value\n","\n","# if you get stuck: a reference implementation is available in SSWYieldModelingTutorial.calc_dMag_from_intTime"]},{"cell_type":"code","execution_count":null,"id":"19ec4504-4027-47ba-a57f-9f950af4d626","metadata":{"id":"19ec4504-4027-47ba-a57f-9f950af4d626"},"outputs":[],"source":["# Now let's check our work.  We should be able to reproduce our dMag input\n","C_p, C_b, M = Cp_Cb_M(static_params, coronagraph, target, 26)\n","SNR = 5\n","intTime = calc_intTime(C_p, C_b, M, SNR)\n","dMag = calc_dMag_from_intTime(static_params, coronagraph, target, SNR, intTime)\n","# should be 26"]},{"cell_type":"markdown","id":"e28cf183-af15-4dd3-9f95-deee3c55bceb","metadata":{"id":"e28cf183-af15-4dd3-9f95-deee3c55bceb"},"source":["# Target List\n","\n","We have now assembled the machinery necessary to compute integration times, achievable $\\Delta\\mathrm{mag}$ values, and completeness values for any arbitrary target.  What we're missing is some actual target data to work with. Let's fix that.  For our target list, we'll use the NASA ExEP Mission Star List for the Habitable Worlds Observatory, compiled by Eric Mamajek and Karl Stapelfeldt and described here: https://arxiv.org/pdf/2402.12414.  A description of the columns in the target list is available here: https://exoplanetarchive.ipac.caltech.edu/docs/API_mission_stars.html\n","\n","We can load the target list via a utility method:"]},{"cell_type":"code","execution_count":null,"id":"3e4eb3cb-9635-4294-8349-23464734e001","metadata":{"id":"3e4eb3cb-9635-4294-8349-23464734e001"},"outputs":[],"source":["targets = SSWYieldModelingTutorial.load_HWO_MissionStars()\n","targets"]},{"cell_type":"markdown","id":"ae903065-405f-4338-bdd2-de345a02a94b","metadata":{"id":"ae903065-405f-4338-bdd2-de345a02a94b"},"source":["Let's use our new tools to compute the completeness we get for each target if we always integrate to hit a $\\Delta\\mathrm{mag}$ of 25 (not e that this will not necessarily be possible for all targets for any given instrument design):"]},{"cell_type":"code","execution_count":null,"id":"df5f231a-525c-4961-b112-2c877cc1d68f","metadata":{"id":"df5f231a-525c-4961-b112-2c877cc1d68f"},"outputs":[],"source":["# get pre-computed completeness grid\n","Cpdf, sax, dMagax = SSWYieldModelingTutorial.load_precomputed_completeness()\n","\n","# get target parameters\n","dist = targets['sy_dist'].values # distances in parsecs\n","lum = 10**targets['st_lum'].values # luminosities (original in log10(L_sun))\n","\n","# IWA/OWA:\n","IWA = 0.075 #arcseconds\n","OWA = 2 #arcseconds\n","projIWA = IWA*dist\n","projOWA = OWA*dist\n","\n","# compute completenesses\n","comps = SSWYieldModelingTutorial.calc_completeness(Cpdf, sax, dMagax, projIWA, projOWA, 25,lum)\n","print(comps.min(),comps.max(), comps.mean())"]},{"cell_type":"markdown","id":"afb04799-9fcf-4a70-bebf-e36596b1b133","metadata":{"id":"afb04799-9fcf-4a70-bebf-e36596b1b133"},"source":["As we can see, different targets can have wildly different completeness values. Let's try to visualize our target list coverage over the sky:"]},{"cell_type":"code","execution_count":null,"id":"b8da496c-708a-4662-bfd9-a7f2a62a9857","metadata":{"id":"b8da496c-708a-4662-bfd9-a7f2a62a9857"},"outputs":[],"source":["ra = targets['ra'].values*np.pi/180 # right ascension in radians\n","ra[ra > np.pi] -= 2*np.pi #wrap at 180 degrees\n","dec = targets['dec'].values*np.pi/180 # declinations in radians\n","\n","fig7 = plt.figure(figsize=(8, 6))\n","ax7 = fig7.add_subplot(111, projection=\"mollweide\")\n","p = ax7.scatter(ra, dec, c=comps)\n","ax7.grid(True)\n","plt.colorbar(p, label=\"Completeness\");"]},{"cell_type":"markdown","id":"400109f2-5561-427a-9886-10061a145322","metadata":{"id":"400109f2-5561-427a-9886-10061a145322"},"source":["Let's also check how long it would actually take to make all these observations:"]},{"cell_type":"code","execution_count":null,"id":"7c58b89a-4087-4531-acab-2d1a83704fd6","metadata":{"id":"7c58b89a-4087-4531-acab-2d1a83704fd6"},"outputs":[],"source":["# we'll use a slightly more realistic coronagraph and observatory this time\n","static_params = {\"lam\": 550*u.nm, # 550 nm central wavelength\n","                 \"deltaLam\": 110*u.nm, # 20% bandpass\n","                 \"D\": 6*u.m, # 6 meter telescope\n","                 \"obsc\": 0.1, # Primary is 10% obscured\n","                 \"tau\": 0.1, # The non-coronagraphic throughput\n","                 \"QE\": 0.9, # 90% Quantum Efficiency\n","                 # Zero-mag flux in photons cm^-2 nm^-1 s^-1 (photons omitted from unit):\n","                 \"F0\": 12000/u.cm**2/u.nm/u.s, # Zero-magnitude flux (approximate)\n","                 \"pixelScale\": 0.002*u.arcsec, # instantaneous field of view of each detector pixel\n","                 # dark current in counts/second/pixel (counts and pixels ommited from unit):\n","                 \"darkCurrent\": 0.001/u.s,\n","                 \"readNoise\": 1e-6, # read noise in electrons/pixel/read\n","                 \"texp\": 100*u.s, # single exposure time\n","                 \"ppFac\": 0.1, # post-processing factor\n","                }\n","\n","coronagraph = {\"tau_core\": 0.1, # point source throughput\n","               \"tau_occ\": 0.2,  # extended source throughput\n","               \"contrast\": 1e-10, # contrast (this represents a pretty good coronagraph)\n","              }\n","\n","# but now we'll do all the targets at once\n","target = {\"mag_star\":  targets['sy_vmag'].values, # apparent magnitude\n","          \"zodi\": 23, # local zodi (units omitted as we're using this as an exponent)\n","          \"exozodi\": 22, # exozodi (assume a bit brighter than local zodi)\n","         }\n","\n","C_p, C_b, M = SSWYieldModelingTutorial.Cp_Cb_M(static_params, coronagraph, target, 25)\n","intTimes = SSWYieldModelingTutorial.calc_intTime(C_p, C_b, M, 5)\n","\n","fig8, ax8 = plt.subplots()\n","ax8.scatter(target[\"mag_star\"], intTimes)\n","ax8.set_yscale(\"log\")\n","ax8.set_xlabel('Target V-band Magnitude')\n","ax8.set_ylabel(\"Required integration time for $\\Delta\\mathrm{mag}=25$ (s)\")\n","ax8.set_title(f\"Total integration time = {intTimes.sum().to(u.yr) :.2f}\");"]},{"cell_type":"markdown","id":"c87755b9-b41c-47fb-9d72-a6cb43c3b15f","metadata":{"id":"c87755b9-b41c-47fb-9d72-a6cb43c3b15f"},"source":["Bottom line: depending on your assumptions about your optical system, it might take you over 6 years (longer than some mission concept total mission durations) to integrator on all of your targets just once!"]},{"cell_type":"markdown","id":"114f95cc-ac4d-4fb0-ba6a-45da71cca0ce","metadata":{"id":"114f95cc-ac4d-4fb0-ba6a-45da71cca0ce"},"source":["# Mission Constraints\n","\n","Finally, on top of everything else we've discussed, there exist mission constraints that dictate when and whether we can make observations of given target stars.  One mission constraint we've already touched upon: the total available time.  Missions are finite, especially those that make use of consumables such as fuel, or cryogens, or anything else that will eventually run out.  Worse yet, integration time for exoplanet observations is only a fraction of the overall mission time.  For each target, there will be some time needed to set up the observation (point at the target, wait for the observatory to equilibrate, dig a dark hole if using a coronagraph, etc.).  There will also be overhead time on each target (time needed to maintain the dark hole or observatory pointing, or to control a starshade, if using one).  All of this adds up quite rapidly.  Worse still: there will be silly people out there who want to use your observatory for science other than exoplanet imaging, and, of course, we have to share. So, even if you have a 5 year mission, you're going to have a lot less time (possibly less than half of that) to get your exoplanet science done.\n","\n","## Keepout Maps\n","In addition to overall mission time constraints, there are specific constraints on when we can observe a given target. The whole point of exoplanet imaging is that we are creating an incredibly dark region in our focal plane in which we can try to collect the scant few photons exoplanets provide us.  If our observatory happens to be pointing in the vicinity of something bright (such as the Sun, or Earth, or even the moon), then that rather defeats the point.  Because of this, there will be so-called **keepout** regions - pointing geometries that are disallowed because they stray too near a bright foreground source. At the same time, depending on how the solar panels are mounted on our observatory, we may have disallowed pointing geometries because they would leave our solar panels unilluminated, meaning that there would not be enough power to run the observatory. Here is a schematic representation of the keepout regions for the sun, for an observatory placed on a Sun-Earth $L_2$ halo orbit (this is a large, periodic (in the rotating or synodic frame) orbit about the Sun-Earth 2nd Lagrange point, such that the sun, Earth, and moon, are always in the same general direction with respect to the observatory):\n","\n","![keepout schematic](../img/keepout_diagram.png)"]},{"cell_type":"markdown","id":"0c67796d-d5be-42d3-920d-561267689f91","metadata":{"id":"0c67796d-d5be-42d3-920d-561267689f91"},"source":["In the diagram, created by Gabriel Soto and published in Soto et al. 2019 (https://arc.aiaa.org/doi/10.2514/1.G003747), $\\mathcal R$ represents the rotating frame, with $\\mathbf{x}$ always pointing from the sun to the Earth (and to the second Lagrange point). The target star cannot lie anywhere within the shaded region to the left of $L_2$ because the sun would shine light into the telescope.  At the same time, a target star cannot lie anywhere within the shaded region to the right of $L_2$ because sunlight would reflect off of the starshade (if using one) and back into the telescope.  Alternatively, if our observatory has solar panels mounted parallel to the boresight (this is the case for the Nancy Grace Roman Space Telescope, for example), then solar panel illumination constraints would produce a keepout that looks a lot like that due to a starshade.\n","\n","We can capture all of this complexity in a **keepout map** - a boolean array where every row represents a target and every column represents some interval of mission time (say one day).  The value of each entry tells us whether the target can be observed on a given day.  Any proposed mission observation schedule must be consistent with the keepout map.\n","\n","Let's take a look at a keepout map for the HWO Mission Star list for a case where the keepout looks like the one in the diagram above."]},{"cell_type":"code","execution_count":null,"id":"518fd63d-f632-4a92-82dc-3c21c834d564","metadata":{"id":"518fd63d-f632-4a92-82dc-3c21c834d564"},"outputs":[],"source":["# load the keepout map\n","# koTimes will be an array of times corresponding to the columns of the keepout map\n","# koMap will have rows of target availability and columns of time:\n","# True values mean that a target can be observed and False means that it can't\n","# targNames will be an array of target names in the same order as the rows of koMap\n","# this should also be the order of the target list we loaded above\n","koTimes, koMap, targNames = SSWYieldModelingTutorial.load_HWO_MissionStars_koMap()\n","\n","# plot the results\n","fig8, ax8 = plt.subplots(1, 1)\n","cmap = matplotlib.colors.ListedColormap([\"black\", \"green\"])\n","p = ax8.pcolor(\n","    koTimes.value,\n","    np.arange(len(targNames)),\n","    koMap,\n","    cmap=cmap,\n",")\n","ax8.set_ylabel(\"Target Number\")\n","ax8.set_xlabel(f\"Time [{koTimes.format}]\")\n","cbar = plt.colorbar(p,ticks=[0.25, 0.75], drawedges=True)\n","cbar.ax.set_yticklabels([\"Unavailable\", \"Available\"]);"]},{"cell_type":"markdown","id":"fed12800-df69-41fb-933f-117b1380ebad","metadata":{"id":"fed12800-df69-41fb-933f-117b1380ebad"},"source":["We can instantly see two things: first, many stars are unavailable for observation much of the time (bummer).  Second: there appears to be some structure to this map.  In fact, there is: the larger the (absolute value) of the ecliptic latitude of the target, the more often it will be observable. At the same time, the target's ecliptic longitude determines when during a calendar year the target will be observable. We can show this by sorting our targets prior to plotting:"]},{"cell_type":"code","execution_count":null,"id":"f450cedd-ab9a-4bed-be85-e4677da4299e","metadata":{"id":"f450cedd-ab9a-4bed-be85-e4677da4299e"},"outputs":[],"source":["# calculate ecliptic target coordinates\n","coords =  SkyCoord(\n","            ra=targets[\"ra\"].values * u.deg,\n","            dec=targets[\"dec\"].values * u.deg,\n","            distance=targets['sy_dist'] * u.pc,\n","        ).heliocentrictrueecliptic\n","\n","sInds = np.argsort(coords.lon)\n","fig9, ax9 = plt.subplots(1, 1)\n","cmap = matplotlib.colors.ListedColormap([\"black\", \"green\"])\n","p = ax9.pcolor(\n","    koTimes.value,\n","    np.arange(len(targNames)),\n","    koMap[sInds],\n","    cmap=cmap,\n",")\n","ax9.set_ylabel(\"Target Number\")\n","ax9.set_xlabel(f\"Time [{koTimes.format}]\")\n","cbar = plt.colorbar(p,ticks=[0.25, 0.75], drawedges=True)\n","cbar.ax.set_yticklabels([\"Unavailable\", \"Available\"]);"]},{"cell_type":"markdown","id":"092b8e9f-c34b-4e65-9483-ce2106b9362b","metadata":{"id":"092b8e9f-c34b-4e65-9483-ce2106b9362b"},"source":["Finally, let's plot the total amount of time each target is available throughout the course of one year:"]},{"cell_type":"code","execution_count":null,"id":"a6192f9c-5874-404b-b9cd-1429242d9bc5","metadata":{"id":"a6192f9c-5874-404b-b9cd-1429242d9bc5"},"outputs":[],"source":["totavail = np.sum(koMap,\n","    axis=1,\n",")\n","plt.figure()\n","plt.scatter(coords.lat, totavail)\n","plt.xlabel(\"Ecliptic Latitude [deg]\")\n","plt.ylabel(\"Total Target Availability [days]\");"]},{"cell_type":"markdown","id":"2d41b0e2-d85f-4020-be2d-3fb8b76e4cf5","metadata":{"id":"2d41b0e2-d85f-4020-be2d-3fb8b76e4cf5"},"source":["So, for this particular observatory design, targets are available no more than approximately 50% of the time (and, for targets near the ecliptic, for as little as about 100 days out of every year).\n","\n","## Exercise 9\n","\n","Let's get some practice using the keepout map.  Write a function that takes a proposed observation of one or more of our target stars (encoded as a start time and observation duration) and evaluates whether a target is available during this time.  Note that as our keepout map is discretized in one day intervals, we will be approximating the target availability somewhat. We will assume that the target is available for observation if it is listed as available (True) in the koMap array for *all* days when an observation is taking pla"]},{"cell_type":"code","execution_count":null,"id":"7bb94373-4cad-45db-bda2-0d754b686233","metadata":{"id":"7bb94373-4cad-45db-bda2-0d754b686233"},"outputs":[],"source":["def check_target_availbility(koTimes, koMap, targetInds, obsStart, obsDurations):\n","    \"\"\"Check whether targets are available (out of keepout) for proposed observations\n","\n","    Args:\n","        koTimes (~astropy.time.Time):\n","            Absolute MJD mission times from start to end in steps of 1 d\n","        koMap (~numpy.ndarray(bool)):\n","            True means a target unobstructed and observable, and False means a\n","            target unobservable due to obstructions in the keepout zone.\n","        targetInds (arraylike):\n","            Indices of targets to check\n","        obsStart (~astropy.time.Time):\n","            Observation start time (absolute time)\n","        obsDurations (Quantity):\n","            Duration of observation(s). Time units.\n","\n","\n","    Returns:\n","        ~numpy.ndarray(bool):\n","            Boolean array of target availability. True means the target is available\n","            for observation and false means it is not. The size of this array should be\n","            the same as the size of targetInds.\n","\n","    .. note::\n","\n","        The size of targetInds and obsDurations must be the same.\n","\n","    \"\"\"\n","\n","    # calculate observation end times:\n","    obsEnds =\n","\n","    # find index  of start time in koTimes\n","    startInd =\n","\n","    # find indices of end times in koTimes\n","    endInds =\n","\n","    # compute availability\n","    # for target j, koMap must be true for *all* indices between\n","    # startInd and endInds[j] for the target to be available\n","    avail =\n","\n","    return avail\n","\n","# if you get stuck: a reference implementation is available in SSWYieldModelingTutorial.check_target_availbility"]},{"cell_type":"code","execution_count":null,"id":"e664343d-97fd-48cc-942b-f0ad6fc3351f","metadata":{"id":"e664343d-97fd-48cc-942b-f0ad6fc3351f"},"outputs":[],"source":["# now let's check our implementation\n","# let's look at three different targets:\n","targetInds = [0, 55, 150]\n","# all observations start at the same time:\n","obsStart = Time(60809.0, format=\"mjd\", scale=\"tai\")\n","# but have different durations:\n","obsDurations = np.array([1, 30, 100])*u.hr\n","avail = check_target_availbility(koTimes, koMap, targetInds, obsStart, obsDurations)\n","# should be True, False, False\n","# The first target is available on the day the observation takes place\n","# The second target is unavailable on both days of the observation\n","# The third target is initially available (first three days) but then becomes unavailable (last 2 days)"]},{"cell_type":"markdown","id":"8890ed33-253b-41b9-ac2f-fb68f6164442","metadata":{"id":"8890ed33-253b-41b9-ac2f-fb68f6164442"},"source":["# Concluding Remarks\n","\n","In this tutorial, we have been introduced to some of the core building blocks of estimating yield for an exoplanet direct imaging mission.  Here, we will briefly discuss how these blocks are put together.  We start with a description of our mission, including an instrument and observatory design, an input target list, and a statement of mission constraints (nominally the total mission time, the fraction of that time available for exoplanet science, and the parameters that allow us to evaluate a keepout map for our target list). We must then generate a mission schedule, consisting of a list of observing times and durations (that is, when we wish to integrate on a given target, and for how long). This schedule must fully conform to our mission constraints (e.g. no looking at stars when they're in keepout, and no exceeding your time allocation).\n","\n","For each observation, we can then compute the maximum achievable planet $\\Delta\\mathrm{mag}$ and from this, the completeness of the observation for a planet population of interest. We sum all of these completeness values together to get the total mission completeness.  This value, in turn, is multiplied by the assumed occurrence rate of planets (frequently called $\\eta$, or $\\eta_\\oplus$ for Earth-like planets specifically). This value:\n","$$\\eta \\sum_i c_i $$\n","where $c_i$ are the completeness values of each observation in our mission schedule, is the expected detection yield of our mission."]}],"metadata":{"colab":{"provenance":[{"file_id":"1oHdmiDH7cUQwi6bwr4oiL4TF6cPcXSys","timestamp":1716597627624}],"toc_visible":true},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":5}